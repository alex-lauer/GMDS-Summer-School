[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Longitudinal Data Modeling",
    "section": "",
    "text": "Preface"
  },
  {
    "objectID": "intro.html#workshop-structure",
    "href": "intro.html#workshop-structure",
    "title": "1  Introduction",
    "section": "1.1 Workshop Structure",
    "text": "1.1 Workshop Structure\nThis class focuses on the longitudinal modeling of data from Patient Reported Outcomes (PROs). It is meant to be hands-on class with applications in R.\nContent and structure follow the book by (Mallinckrodt 2016). We would like to extend our warmest gratitude towards Dr. Mallinckrodt for providing the example data for the workshop.\nThe following topics will be covered:\n\nWelcome and Introduction (WS session 1)\nExploration and visualization of longitudinal data (WS session 1/2)\nInferences from longitudinal data (WS session 3 + 4)\nAssessment of missingness patterns (WS session 5)\nSensitivity analyses to assess the impact of missingness (WS session 6)\nAnnex: Inferences from longitudinal binary data (WS session 7)"
  },
  {
    "objectID": "intro.html#longitudinal-data",
    "href": "intro.html#longitudinal-data",
    "title": "1  Introduction",
    "section": "1.2 Longitudinal Data",
    "text": "1.2 Longitudinal Data\nThis workshop focuses on the analysis of data observed in randomized clinical trials (RCTs). Here, patients have assessments taken at the start of their treatment and then subsequently throughout the course of the trial based on a pre-specified schedule of assessments. The measurement at the start of the treatment is usually referred to as the baseline.\nResearchers can be interested in\n\nthe occurrence of a certain event during the course of the trial, e.g. death or a cardiac event, or the time to the occurrence of such an event, or\nthe longitudinal profile from multiple repeated measurements taken, with a focus on either estimates at a landmark visit or across several time points.\n\nThe outcomes under point 1. can be handled via a comparison of the percentages of patients with events between treatment arms, or a time-to-event analysis. Both are out of scope of this workshop."
  },
  {
    "objectID": "intro.html#basics-about-rstudio-pre-read",
    "href": "intro.html#basics-about-rstudio-pre-read",
    "title": "1  Introduction",
    "section": "1.3 Basics about RStudio (pre-read)",
    "text": "1.3 Basics about RStudio (pre-read)\nAlex to add pre-read (YouTube + Cheat Sheets)"
  },
  {
    "objectID": "intro.html#example-data",
    "href": "intro.html#example-data",
    "title": "1  Introduction",
    "section": "1.4 Example data",
    "text": "1.4 Example data\n\n\n\n\nMallinckrodt, Craig. 2016. Analyzing Longitudinal Clinical Trial Data. Chapman; Hall/CRC. https://doi.org/10.1201/9781315186634."
  },
  {
    "objectID": "s1_visualization.html",
    "href": "s1_visualization.html",
    "title": "2  Longitudinal Data Exploration and Visualization",
    "section": "",
    "text": "3 Correlation structure, covariance matrices\nThis technical detour is motivated by (Fitzmaurice 2011). Let us assume we are only interested in the first two responses in a clinical study, say Visit 1 (Baseline) and Visit 2. Out interest lies in an assessment of mean changes over time (for the sake of simplicity in a single treatment group only), i.e. we wish to estimate\n\\[\n\\hat\\delta := \\hat\\mu_2 - \\hat\\mu_1 = \\frac{1}{N} \\sum_{i=1}^N (Y_{i2} - Y_{i1})\\,,\n\\]\nwhere \\(Y_{i1}\\) and \\(Y_{i2}\\) are observations from subject \\(i\\) at Visit 1 and Visit 2, respectively. To obtain the standard error (SE) and get a notion of variability, we compute the variance of \\(\\hat\\delta\\) and see that\n\\[\n\\text{Var}(\\hat\\delta) = \\text{Var}\\left(\\frac{1}{N} \\sum_{i=1}^N (Y_{i2} - Y_{i1})\\right) = \\frac{1}{N} (\\sigma_1^2 + \\sigma_2^2 - 2\\sigma_{12})\\,.\n\\]\nThe inclusion of the term \\(- 2\\sigma_{12}\\) accounts for the correlation between responses at Visit 1 and Visit 2. As data from adjacent visits is usually positively correlated, the omission of the correlation term leads to an overestimation of the variance and thus the SE associated with the treatment effect."
  },
  {
    "objectID": "s1_visualization.html#introduction",
    "href": "s1_visualization.html#introduction",
    "title": "2  Longitudinal Data Exploration and Visualization",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\n\nData on individuals followed over time with information collected at several time points.\nClusters are the individuals who are followed over time.\nRepeated observations may or may not be taken at regular times (balanced, fixed occasions, do not differ between subjects).\nOur interest is in the change from baseline.\n\nDatasets used in this course: - Example data is taken from (Mallinckrodt 2016). The authors generated data sets based on two nearly identically designed antidepressant clinical trials by randomly selecting subjects from the original data. - Contain data on the continuous variable HAMD17 (Hamilton 17-item rating scale for depression). - Two treatement arms are included: placebo (arm 1) vs. drug (arm 2). - Assessments were taken at baseline and weeks 1, 2, 4, 6, and 8.\nThere are 3 data sets created from the original data: - Data all2 = Subsample of the large dataset with n=50, visits: weeks 2, 4, 8 - Data high2 = Large dataset with n=100, high dropout = 70% (drug), 60% (placebo) - Data low2 = Large dataset with n=100, low dropout = 18%"
  },
  {
    "objectID": "s1_visualization.html#data-set-all2",
    "href": "s1_visualization.html#data-set-all2",
    "title": "2  Longitudinal Data Exploration and Visualization",
    "section": "2.2 Data set all2",
    "text": "2.2 Data set all2\n\nSmall data set with n=50 subjects.\n1st version: complete data where all subjects adhered to the originally assigend study medication, variable change\n2nd version = missing data: identical to the first except some data were missing (drop-out), variable chgdrop\n\nLooking at the variables in the data set\n\nhead(all2)\n\n# A tibble: 6 × 14\n  subject  time chgdrop trt   basval change pgiimp gender chgrescue dropout_grp \n  &lt;fct&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;       \n1 1           1     -11 2         24    -11      3 F            -11 Week 2 Drop…\n2 1           2      NA 2         24    -16      2 F            -26 Week 2 Drop…\n3 1           3      NA 2         24    -24      2 F            -34 Week 2 Drop…\n4 2           1      -6 1         20     -6      4 F             -6 Week 2 Drop…\n5 2           2      NA 1         20     -8      4 F            -18 Week 2 Drop…\n6 2           3      NA 1         20     -5      5 F            -15 Week 2 Drop…\n# ℹ 4 more variables: aval &lt;dbl&gt;, avisit &lt;fct&gt;, week &lt;dbl&gt;, group &lt;fct&gt;\n\n\n\n2.2.1 Task 1 - Exploration of data set all2 - 15 minutes working time\nOnly consider the complete data, variable change - Are the data balanced and equally spaced? - Number of observations by week? - Summary statistics for HAMD17 (change from baseline) by week. - Plot trajectories for each individual, different colors for each treatment group (or panels). - Add mean to your plot or generate new plot with mean change from baseline by treatment group. - Plot mean change from baseline for each treatment group stratified by sex. Comment on the plot.\n\n\n2.2.2 Task 1 Discussion, possible solution\nTable: Summary statistics for HAMD17 by treatment and week in the all2 data set\n\nall2 %&gt;%\n  select(change, group, avisit) %&gt;%\n  tbl_strata(strata=group, \n             ~.x %&gt;% \n               tbl_summary(by = avisit,\n                           statistic = list(\n      all_continuous() ~ \"{mean} ({sd})\"), \n      digits = all_continuous() ~ 2 ) \n)\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      \n        Arm 1\n      \n      \n        Arm 2\n      \n    \n    \n      Week 2, N = 251\n      Week 4, N = 251\n      Week 8, N = 251\n      Week 2, N = 251\n      Week 4, N = 251\n      Week 8, N = 251\n    \n  \n  \n    change\n-4.20 (3.66)\n-6.80 (4.25)\n-9.88 (4.85)\n-5.24 (5.49)\n-8.60 (5.39)\n-13.24 (5.54)\n  \n  \n  \n    \n      1 Mean (SD)\n    \n  \n\n\n\n\nFigure: individual trajectories stratified by treatment group\n\nggplot(data = all2, aes(x = week, y = change, group=subject)) +\n  geom_point() + geom_line() + facet_grid(.~group) + ylab(\"Change from baseline HAMD17\") +\n  scale_x_continuous(name=\"Visit [week]\", breaks=c(2,4,8))\n\n\n\n\nFigure 2.1: Individual trajectories of HAMD17 by treatment group\n\n\n\n\nFigure: Mean change from baseline for each treatment group\n\nggplot(data = all2, aes(x = week, y = change)) +  \n  geom_point(aes(colour=factor(group))) + ylab(\"Change from baseline HAMD17\") +\n  scale_x_continuous(name=\"Visit [week]\", breaks=c(2,4,8)) +\n  stat_summary(aes(group = group, colour=factor(group)), geom = \"line\", fun.y = mean,\n               size = 1) +\n  stat_summary(aes(group = group, colour=factor(group)), geom = \"point\", fun.y = mean,\n               shape=17,size = 2)\n\nWarning: The `fun.y` argument of `stat_summary()` is deprecated as of ggplot2 3.3.0.\nℹ Please use the `fun` argument instead.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nFigure 2.2: Mean HAMD17 change from baseline by treatment group\n\n\n\n\nFrequency for sex per treatment group\n\nall2 %&gt;% filter(time==1) %&gt;%\n  tbl_summary(\n    include = c(gender),\n    by = group\n  )\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      Arm 1, N = 251\n      Arm 2, N = 251\n    \n  \n  \n    PATIENT SEX\n\n\n        F\n10 (40%)\n19 (76%)\n        M\n15 (60%)\n6 (24%)\n  \n  \n  \n    \n      1 n (%)\n    \n  \n\n\n\n\nFigure: Mean change from baseline stratified by sex\n\nggplot(data = all2, aes(x = week, y = change)) +  facet_grid(.~gender) +\n  geom_point(aes(colour=factor(group))) + ylab(\"Change from baseline HAMD17\") +\n  scale_x_continuous(name=\"Visit [week]\", breaks=c(2,4,8)) +\n  stat_summary(aes(group = group, colour=factor(group)), geom = \"line\", fun.y = mean,\n               size = 1) +\n  stat_summary(aes(group = group, colour=factor(group)), geom = \"point\", fun.y = mean,\n               shape=17,size = 2)\n\n\n\n\nFigure 2.3: Mean HAMD17 change from baseline by treatment group stratified by sex\n\n\n\n\n\n\n2.2.3 Data set all2 with drop-out\n\n2nd version = missing data: identical to the first except some data were missing (drop-out), variable chgdrop\nThis version is later relevant when considering missing data. Thus, have a short look at the data.\n\nTable: Summary statistics for HAMD17 by treatment and week in the all2 data set with drop-outs\n\nall2 %&gt;%\n  select(chgdrop, group, avisit) %&gt;%\n  tbl_strata(strata=group, \n             ~.x %&gt;% \n               tbl_summary(by = avisit,\n                           statistic = list(\n      all_continuous() ~ \"{mean} ({sd})\"), \n      digits = all_continuous() ~ 2 ) \n)\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      \n        Arm 1\n      \n      \n        Arm 2\n      \n    \n    \n      Week 2, N = 251\n      Week 4, N = 251\n      Week 8, N = 251\n      Week 2, N = 251\n      Week 4, N = 251\n      Week 8, N = 251\n    \n  \n  \n    chgdrop\n-4.20 (3.66)\n-6.80 (4.63)\n-10.17 (4.88)\n-5.24 (5.49)\n-8.14 (5.27)\n-13.11 (5.44)\n        Unknown\n0\n5\n7\n0\n3\n6\n  \n  \n  \n    \n      1 Mean (SD)"
  },
  {
    "objectID": "s1_visualization.html#data-set-high2",
    "href": "s1_visualization.html#data-set-high2",
    "title": "2  Longitudinal Data Exploration and Visualization",
    "section": "2.3 Data set high2",
    "text": "2.3 Data set high2\n\nLarge data set with n=100 subjects.\nNote that we have no intermittent missing values but drop-outs.\n\nLooking at the variables in the data set.\n\nhead(high2)\n\n# A tibble: 6 × 15\n# Groups:   patient [2]\n  patient trt   poolinv basval  week change pgiimp   age gender  drop  aval\n    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1    1401 1     005         19     1     -7      3  44.5 F          2    12\n2    1401 1     005         19     2     -4      3  44.5 F          2    15\n3    1411 2     005         17     1      0      3  35.7 F          8    17\n4    1411 2     005         17     2     -2      3  35.7 F          8    15\n5    1411 2     005         17     4      2      3  35.7 F          8    19\n6    1411 2     005         17     6     -3      2  35.7 F          8    14\n# ℹ 4 more variables: group &lt;fct&gt;, avisit &lt;fct&gt;, dropout_grp &lt;fct&gt;,\n#   subject &lt;fct&gt;\n\n\n\n2.3.1 Task 2 - Exploration of data set high2 - 15 minutes working time\n\nDrop-outs, last visit for each subject: number of observations by week.\nSummary statistics for HAMD17 change.\nGenerate and interpret the group-wise boxplots of the change from baseline.\nMean change from baseline for different drop-out groups (by treatment). Comment on the plot.\n\n\n\n2.3.2 Task 2 Discussion, possible solution\nTable: Summary statistics for HAMD17 by treatment and week in the high2 data set\n\nhigh2 %&gt;% ungroup() %&gt;%\n  select(change, group, avisit) %&gt;%\n  tbl_strata(strata=group, \n             ~.x %&gt;% \n               tbl_summary(by = avisit,\n                           statistic = list(\n                             all_continuous() ~ \"{mean} ({sd})\"), \n                           digits = all_continuous() ~ 2 ) \n  )\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      \n        Arm 1\n      \n      \n        Arm 2\n      \n    \n    \n      Week 1, N = 1001\n      Week 2, N = 921\n      Week 4, N = 851\n      Week 6, N = 731\n      Week 8, N = 601\n      Week 1, N = 1001\n      Week 2, N = 901\n      Week 4, N = 851\n      Week 6, N = 751\n      Week 8, N = 701\n    \n  \n  \n    change\n-1.49 (3.91)\n-3.16 (5.69)\n-4.51 (6.23)\n-5.51 (6.16)\n-6.58 (5.99)\n-1.84 (5.58)\n-4.30 (6.82)\n-6.47 (6.84)\n-8.29 (6.96)\n-8.99 (7.04)\n  \n  \n  \n    \n      1 Mean (SD)\n    \n  \n\n\n\n\nFigure: Distribution of HAMD17 change from baseline\n\nggplot(data = high2, aes(x = avisit, y = change, fill=group)) + \n  geom_boxplot() + ylab(\"Change from baseline HAMD17\") + xlab(\"Visit\")\n\n\n\n\nFigure 2.4: Distribution of HAMD17 change from baseline by treatment group at each visit\n\n\n\n\nFigure: Mean HAMD17 changes by drop-out group\n\nggplot(data = high2, aes(x = week, y = change, group=patient)) + \n  geom_point(col=\"lightgray\") + geom_line(col=\"lightgray\") + facet_grid(.~group) +\n  ylab(\"Change from baseline HAMD17\") + scale_x_continuous(name=\"Visit [week]\", breaks=c(1,2,4,6,8)) +\n  stat_summary(aes(group = dropout_grp, colour=factor(dropout_grp)), geom = \"line\", fun.y = mean,\n               size = 1) +\n  stat_summary(aes(group = dropout_grp, colour=factor(dropout_grp)), geom = \"point\", fun.y = mean,\n               shape=17,size = 2)\n\n\n\n\nFigure 2.5: Visit-wise mean HAMD17 changes from baseline by treatment group and drop-out"
  },
  {
    "objectID": "s1_visualization.html#overview---different-covariance-matrices",
    "href": "s1_visualization.html#overview---different-covariance-matrices",
    "title": "2  Longitudinal Data Exploration and Visualization",
    "section": "3.1 Overview - different covariance matrices",
    "text": "3.1 Overview - different covariance matrices\n\nVariance components (VC) independence structure\nCompound symmetry (CS) also known as exchangeable\nToeplitz (TOEP)\nFirst order auto regressive (AR(1))\nUnstructured (UN)\n\nSelected covariance structures for data with three assessment times (t=3) are shown below. Note that with three assessment times, the number of parameters estimated for the various structures did not differ as much as would be the case with more assessment times. Thus, results from different covariance structures are more similar than would be the case with more assessment times.\n\n3.1.1 Independence structure (VC)\nConstant variance. It is assumed to be no correlation between assessments (residuals are independent across time). \\[ R = \\begin{bmatrix}\n   \\sigma^2   & 0  & 0  \\\\\n   0  & \\sigma^2   & 0 \\\\\n   0  & 0  & \\sigma^2\n   \\end{bmatrix}\\]\n\n\n3.1.2 Compound symmetry (CS)\nConstant variance and constant covariance across all assessments. Also known as exchangeable. It requires two parameter estimates. Most simplest repeated measures (i.e., correlated errors) structure.\n\\[ R = \\begin{bmatrix}\n   \\sigma^2 + \\sigma_1 & \\sigma_1  & \\sigma_1  \\\\\n   \\sigma_1  & \\sigma^2 + \\sigma_1  & \\sigma_1  \\\\\n   \\sigma_1  & \\sigma_1  & \\sigma^2 + \\sigma_1\n   \\end{bmatrix}\\]\n\n\n3.1.3 Unstructured (UN)\nThis is the most general (saturated) model. It has t + [t(t-1)/2] parameters to be estimated. Here it is 3 + 3 = 6 parameters.\n\\[ R = \\begin{bmatrix}\n   \\sigma_1^2 & \\sigma_{21}  & \\sigma_{31}  \\\\\n   \\sigma_{21}  & \\sigma_2^2   & \\sigma_{32}  \\\\\n   \\sigma_{31}  & \\sigma_{32}  & \\sigma_3^2\n   \\end{bmatrix}\\]\n\n\n3.1.4 Toeplitz structure (TOEP)\nHomogenous variances and heterogenous correlations. Same correlation value is used whenever the degree of adjacency is the same e.g. correlation between times 1 and 2 = correlation between times 2 and 3. Repeated measurements are assumed to be equally spaced. TOEP requries t parameter estimates so here we have t=3 parameter.\n\\[ R = \\begin{bmatrix}\n   \\sigma^2  & \\sigma_1^2 & \\sigma_2^2 \\  \\\\\n   \\sigma_1^2 & \\sigma^2  & \\sigma_1^2  \\\\\n   \\sigma_2^2  & \\sigma_1^2 & \\sigma^2\n   \\end{bmatrix}\\]\n\n\n3.1.5 Autoregressive structure (AR(1))\nCorrelation decreases as time between observations increases. Assumtpion of equal spacing between each repeated measurement must be reasonably applicable. This structure requires the estimation of two parameters.\n\\[ R = \\begin{bmatrix}\n   \\sigma^2  & \\sigma^2 \\rho  & \\sigma^2 \\rho^2  \\\\\n   \\sigma^2 \\rho & \\sigma^2   & \\sigma^2 \\rho  \\\\\n  \\sigma^2 \\rho^2  & \\sigma^2 \\rho  & \\sigma^2\n   \\end{bmatrix}\\]\n\n\n3.1.6 Spatial Power (SP)\nSpatial covariance structures does not require equal spacing between measurements. Instead, as long as the distance between visits can be quantified in terms of time and/or other coordinates, the spatial covariance structure can be applied. Covariances are mathematical functions of Euclidean distances between observed measurements. Again, two parameters need to be estimated.\nFor spatial exponential, the covariance structure is defined as follows:\n\\[ R = \\begin{bmatrix}\n   \\sigma^2  & \\sigma^2 \\rho_{12}  & \\sigma^2 \\rho_{13}  \\\\\n   \\sigma^2 \\rho_{21} & \\sigma^2   & \\sigma^2 \\rho_{23}  \\\\\n  \\sigma^2 \\rho_{31}  & \\sigma^2 \\rho_{32}  & \\sigma^2\n   \\end{bmatrix}\\]\nwith \\[ \\rho_{ij}=\\rho^{d_{ij}} \\] where \\[d_{ij} \\] is the distance between time point i and time point j e.g. distance in weeks."
  },
  {
    "objectID": "s1_visualization.html#selecting-the-covariance-structure",
    "href": "s1_visualization.html#selecting-the-covariance-structure",
    "title": "2  Longitudinal Data Exploration and Visualization",
    "section": "3.2 Selecting the covariance structure",
    "text": "3.2 Selecting the covariance structure\nThere are a variety of considerations when selecting the covariance structure: - number of parameters - interpretation of the structure - model fit\nUN is the most flexible (complex) structure and can fail to run especially if one has many repeated measures. Choose a reasonable covaraiance structure which is the best compromise between model fit and complexity. E.g. use AIC as it penalises more complex models."
  },
  {
    "objectID": "s1_visualization.html#task-3---exploration-of-correlation-in-the-data",
    "href": "s1_visualization.html#task-3---exploration-of-correlation-in-the-data",
    "title": "2  Longitudinal Data Exploration and Visualization",
    "section": "3.3 Task 3 - Exploration of correlation in the data",
    "text": "3.3 Task 3 - Exploration of correlation in the data\n\nCompute the empirical correlations between measurement timepoints in the all2 data set (e.g. correlation between baseline and post-baseline changes).\nLooking at these correlations + using your knowledge of the experiment (e.g., spacing of measurements), comment on the suitability of the correlation structures VC, CS, UN, AR(1)."
  },
  {
    "objectID": "s1_visualization.html#task-3---discussion-and-possible-solution",
    "href": "s1_visualization.html#task-3---discussion-and-possible-solution",
    "title": "2  Longitudinal Data Exploration and Visualization",
    "section": "3.4 Task 3 - Discussion and possible solution",
    "text": "3.4 Task 3 - Discussion and possible solution\nTable: Correlation and covariance matrix\n\nall2 %&gt;% pivot_wider(id_cols=subject,names_from = time, values_from = c(basval,change)) %&gt;% \n  select(-c(basval_2,basval_3)) -&gt; all2.w\n\ncor(all2.w[-1]) \n\n            basval_1   change_1   change_2    change_3\nbasval_1  1.00000000 -0.2636447 -0.3165711 -0.02915138\nchange_1 -0.26364471  1.0000000  0.7557078  0.51502724\nchange_2 -0.31657106  0.7557078  1.0000000  0.71298768\nchange_3 -0.02915138  0.5150272  0.7129877  1.00000000\n\ncov(all2.w[-1])\n\n           basval_1  change_1  change_2   change_3\nbasval_1 16.3330612 -4.955918 -6.253061 -0.6391837\nchange_1 -4.9559184 21.634286 17.179592 12.9967347\nchange_2 -6.2530612 17.179592 23.887755 18.9061224\nchange_3 -0.6391837 12.996735 18.906122 29.4351020"
  },
  {
    "objectID": "s2_inference_continuous.html#categorical-time",
    "href": "s2_inference_continuous.html#categorical-time",
    "title": "3  Inference from Longitudinal Data",
    "section": "3.1 Categorical Time",
    "text": "3.1 Categorical Time\nYou can start and familiarise yourself with the main function mmrm() using the command\n\nlibrary(mmrm)\n?mmrm\n\nTwo inputs are strictly required to get mmrm() to work:\n\nA model formula\nThe dataset, containing the response, as well as all fixed effects and variables in the covariance matrix.\n\nExercise: Fit a model fit_cat_time using the dataset all2, with change as dependent variable, baseline value, visit, baseline by visit interaction and treatment by visit interaction as fixed effects and an unstructured covariance matrix for visits within each subject.\n\nHow do different choices for covariance matrices change the results? What is the difference on the estimation procedure?\nYou can obtain a summary of the fit results via summary(fit_cat_time). How do you interpret the fit summary?\nLook at the structure of the fit summary and try to extract the estimate of the \\(R\\) matrix.\n\n\nfit_cat_time &lt;- mmrm::mmrm(\n  formula = change ~ basval*avisit + trt*avisit + us(avisit | subject),\n  data = all2,\n  control = mmrm_control(method = \"Kenward-Roger\")\n)\n\nsummary(fit_cat_time)\n\nmmrm fit\n\nFormula:     change ~ basval * avisit + trt * avisit + us(avisit | subject)\nData:        all2 (used 150 observations from 50 subjects with maximum 3 \ntimepoints)\nCovariance:  unstructured (6 variance parameters)\nMethod:      Kenward-Roger\nVcov Method: Kenward-Roger\nInference:   REML\n\nModel selection criteria:\n     AIC      BIC   logLik deviance \n   822.4    833.9   -405.2    810.4 \n\nCoefficients: \n                     Estimate Std. Error        df t value Pr(&gt;|t|)   \n(Intercept)           1.98452    3.27479  47.00000   0.606  0.54743   \nbasval               -0.31235    0.15905  47.00000  -1.964  0.05548 . \navisitWeek 4         -0.90862    2.39866  47.00000  -0.379  0.70654   \navisitWeek 8        -10.58630    3.45922  47.00000  -3.060  0.00365 **\ntrt2                 -1.18993    1.27265  47.00000  -0.935  0.35457   \nbasval:avisitWeek 4  -0.08542    0.11650  47.00000  -0.733  0.46704   \nbasval:avisitWeek 8   0.24779    0.16801  47.00000   1.475  0.14691   \navisitWeek 4:trt2    -0.80100    0.93217  47.00000  -0.859  0.39454   \navisitWeek 8:trt2    -2.20106    1.34432  47.00000  -1.637  0.10825   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCovariance estimate:\n        Week 2  Week 4  Week 8\nWeek 2 20.6112 15.3034 12.2766\nWeek 4 15.3034 21.3565 17.6648\nWeek 8 12.2766 17.6648 27.6127\n\n\nWe can assess the structure of the fit summary via\n\nstr(summary(fit_cat_time))\n\nList of 15\n $ cov_type        : chr \"us\"\n $ reml            : logi TRUE\n $ n_groups        : int 1\n $ n_theta         : int 6\n $ n_subjects      : int 50\n $ n_timepoints    : int 3\n $ n_obs           : int 150\n $ beta_vcov       : num [1:9, 1:9] 10.724 -0.501 -2.675 -4.267 -1.047 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:9] \"(Intercept)\" \"basval\" \"avisitWeek 4\" \"avisitWeek 8\" ...\n  .. ..$ : chr [1:9] \"(Intercept)\" \"basval\" \"avisitWeek 4\" \"avisitWeek 8\" ...\n $ varcor          : num [1:3, 1:3] 20.6 15.3 12.3 15.3 21.4 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:3] \"Week 2\" \"Week 4\" \"Week 8\"\n  .. ..$ : chr [1:3] \"Week 2\" \"Week 4\" \"Week 8\"\n $ method          : chr \"Kenward-Roger\"\n $ vcov            : chr \"Kenward-Roger\"\n $ coefficients    : num [1:9, 1:5] 1.985 -0.312 -0.909 -10.586 -1.19 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:9] \"(Intercept)\" \"basval\" \"avisitWeek 4\" \"avisitWeek 8\" ...\n  .. ..$ : chr [1:5] \"Estimate\" \"Std. Error\" \"df\" \"t value\" ...\n $ n_singular_coefs: int 0\n $ aic_list        :List of 4\n  ..$ AIC     : num 822\n  ..$ BIC     : num 834\n  ..$ logLik  : num -405\n  ..$ deviance: num 810\n $ call            : language mmrm::mmrm(formula = change ~ basval * avisit + trt * avisit + us(avisit |      subject), data = all2, control = | __truncated__\n - attr(*, \"class\")= chr \"summary.mmrm\"\n\n\nand then extract the covariance matrix\n\nsummary(fit_cat_time)$varcor\n\n         Week 2   Week 4   Week 8\nWeek 2 20.61117 15.30339 12.27661\nWeek 4 15.30339 21.35648 17.66478\nWeek 8 12.27661 17.66478 27.61271"
  },
  {
    "objectID": "s2_inference_continuous.html#continuous-time",
    "href": "s2_inference_continuous.html#continuous-time",
    "title": "3  Inference from Longitudinal Data",
    "section": "3.2 Continuous Time",
    "text": "3.2 Continuous Time\nTime as continuous effect -&gt; single df for time and trt-by-time interaction\nModeling: - Need avisit for structure of covariance matrix - Implicit assumption is for the covariance between values for two timepoints to be equal, regardless of the specific timing\n\nfit_cont_time &lt;- mmrm::mmrm(\n  formula = change ~ basval*time + trt*time + us(avisit | subject),\n  weights = all2$time,\n  data = all2,\n  control = mmrm_control(method = \"Kenward-Roger\")\n)\n\nQuadratic trend\n\nall2$timesq &lt;- all2$time^2\n\nfit_cont_timesq &lt;- mmrm::mmrm(\n  formula = change ~ basval*timesq + trt*timesq + us(avisit | subject),\n  weights = all2$time,\n  data = all2,\n  control = mmrm_control(method = \"Kenward-Roger\")\n)\n\nmodel checks - residuals per time point"
  },
  {
    "objectID": "s2_inference_continuous.html#baseline-as-a-response-clda-lda",
    "href": "s2_inference_continuous.html#baseline-as-a-response-clda-lda",
    "title": "3  Inference from Longitudinal Data",
    "section": "3.3 Baseline as a Response (cLDA + LDA)",
    "text": "3.3 Baseline as a Response (cLDA + LDA)"
  },
  {
    "objectID": "s2_inference_continuous.html#adjusted-ls-means-from-mmrms",
    "href": "s2_inference_continuous.html#adjusted-ls-means-from-mmrms",
    "title": "3  Inference from Longitudinal Data",
    "section": "3.4 (Adjusted) LS Means from MMRMs",
    "text": "3.4 (Adjusted) LS Means from MMRMs\nLS Means are means of the dependent variable adjusted for covariates in the statistical model. We can obtain LS Means estimates and contrasts allowing for a treatment comparison using the emmeans package.\nExample: Calculate the observed (raw) means of changes along with number of patients by treatment group from the dataset all2 overall and by visit. Then take the model fit_cat_time and derive the respective LS Means from the model. What do you observe?\n\n# Raw means\n\nall2 %&gt;% \n  dplyr::group_by(group) %&gt;% \n  dplyr::summarise(\n    N = dplyr::n(),\n    Mean = mean(change),\n    .groups = \"drop\"\n  )\n\n# A tibble: 2 × 3\n  group     N  Mean\n  &lt;fct&gt; &lt;int&gt; &lt;dbl&gt;\n1 Arm 1    75 -6.96\n2 Arm 2    75 -9.03\n\nall2 %&gt;% \n  dplyr::group_by(group, avisit) %&gt;% \n  dplyr::summarise(\n    N = dplyr::n(),\n    Mean = mean(change),\n    .groups = \"drop\"\n  )\n\n# A tibble: 6 × 4\n  group avisit     N   Mean\n  &lt;fct&gt; &lt;fct&gt;  &lt;int&gt;  &lt;dbl&gt;\n1 Arm 1 Week 2    25  -4.2 \n2 Arm 1 Week 4    25  -6.8 \n3 Arm 1 Week 8    25  -9.88\n4 Arm 2 Week 2    25  -5.24\n5 Arm 2 Week 4    25  -8.6 \n6 Arm 2 Week 8    25 -13.2 \n\n\nThe respective LS Means from the model with time as a fixed factor yields the following estimates:\n\nlibrary(emmeans)\n\nemmeans::ref_grid(fit_cat_time)\n\n'emmGrid' object with variables:\n    basval = 19.56\n    avisit = Week 2, Week 4, Week 8\n    trt = 1, 2\n\nemmeans(fit_cat_time, ~trt)\n\nNOTE: Results may be misleading due to involvement in interactions\n\n\n trt emmean    SE df lower.CL upper.CL\n 1    -6.90 0.836 47    -8.58    -5.22\n 2    -9.09 0.836 47   -10.77    -7.41\n\nResults are averaged over the levels of: avisit \nConfidence level used: 0.95 \n\nemmeans(fit_cat_time, ~trt*avisit)\n\n trt avisit emmean    SE df lower.CL upper.CL\n 1   Week 2  -4.13 0.899 47    -5.93    -2.32\n 2   Week 2  -5.31 0.899 47    -7.12    -3.51\n 1   Week 4  -6.70 0.916 47    -8.55    -4.86\n 2   Week 4  -8.70 0.916 47   -10.54    -6.85\n 1   Week 8  -9.86 1.033 47   -11.94    -7.79\n 2   Week 8 -13.26 1.033 47   -15.33   -11.18\n\nConfidence level used: 0.95 \n\n\n\n3.4.1 Observed vs. balanced margins\nIn the example above we have used the standard option for the weights in the calculation of LS Means. We will delve deeper into the following two options and will try to understand the difference:\n\nweights = \"equal\": Each stratum induced by covariate levels is assigned the same weight in the calculation of the LS Means. This is the default option.\nweights = \"proportional\": Each stratum induced by covariate levels is assigned a weight according to their observed proportion in the calculation of the LS Mean. This option gives each stratum a weight corresponding to its size. Estimates using this option are reflective of the balance of covariates in the data.\n\nExercise: Based on the fit_cat_time model, compare the LS Means for the change in the response variable by treatment overall and treatment by visit interaction using the different options for weight. Compare the results for the two LS Means options to the observed means and to one another.\nDiscuss the following points:\n\nWhy is there no difference between LS Means estimates for the overall treatment effect and the treatment by visit interaction? (Hint: Create a frequency table)\n\nNow update the fit_cat_time model to fit_cat_time2, and include the covariate gender. Estimate the same LS Means for the change in the response variable by treatment (overall) and treatment by visit interaction.\n\nWhy is there a difference now between results from the different LS Means options? (Hint: another frequency table can help)\nWhat effect could missing data have on the estimation, even in the case of fit_cat_time? I.e. what would happen if this data was not complete but subject to missingness, with the degree of missing data increasing over time and being disproportionate between treatment arms?\n\nSolution:\nWe first calculate the LS Means, using the different weights options and find they are indeed identical.\n\n# These will yield the same results:\nemmeans(fit_cat_time, ~trt, weights = \"equal\")\n\nNOTE: Results may be misleading due to involvement in interactions\n\n\n trt emmean    SE df lower.CL upper.CL\n 1    -6.90 0.836 47    -8.58    -5.22\n 2    -9.09 0.836 47   -10.77    -7.41\n\nResults are averaged over the levels of: avisit \nConfidence level used: 0.95 \n\nemmeans(fit_cat_time, ~trt, weights = \"proportional\")\n\nNOTE: Results may be misleading due to involvement in interactions\n\n\n trt emmean    SE df lower.CL upper.CL\n 1    -6.90 0.836 47    -8.58    -5.22\n 2    -9.09 0.836 47   -10.77    -7.41\n\nResults are averaged over the levels of: avisit \nConfidence level used: 0.95 \n\nemmeans(fit_cat_time, ~trt*avisit, weights = \"equal\")\n\n trt avisit emmean    SE df lower.CL upper.CL\n 1   Week 2  -4.13 0.899 47    -5.93    -2.32\n 2   Week 2  -5.31 0.899 47    -7.12    -3.51\n 1   Week 4  -6.70 0.916 47    -8.55    -4.86\n 2   Week 4  -8.70 0.916 47   -10.54    -6.85\n 1   Week 8  -9.86 1.033 47   -11.94    -7.79\n 2   Week 8 -13.26 1.033 47   -15.33   -11.18\n\nConfidence level used: 0.95 \n\nemmeans(fit_cat_time, ~trt*avisit, weights = \"proportional\")\n\n trt avisit emmean    SE df lower.CL upper.CL\n 1   Week 2  -4.13 0.899 47    -5.93    -2.32\n 2   Week 2  -5.31 0.899 47    -7.12    -3.51\n 1   Week 4  -6.70 0.916 47    -8.55    -4.86\n 2   Week 4  -8.70 0.916 47   -10.54    -6.85\n 1   Week 8  -9.86 1.033 47   -11.94    -7.79\n 2   Week 8 -13.26 1.033 47   -15.33   -11.18\n\nConfidence level used: 0.95 \n\n\nNow we can update the model to include the covariate gender. We can specify this a new model using the mmrm() function again, or simply use update() to add the new covariate to the model. Either way is fine, and a look into the model formula from the fit summary shows the two approaches work interchangeably.\n\nfit_cat_time2 &lt;- update(fit_cat_time, . ~ . + gender)\nsummary(fit_cat_time2)\n\nmmrm fit\n\nFormula:     \nchange ~ basval + avisit + trt + (us(avisit | subject)) + gender +  \n    basval:avisit + avisit:trt\nData:        all2 (used 150 observations from 50 subjects with maximum 3 \ntimepoints)\nCovariance:  unstructured (6 variance parameters)\nMethod:      Kenward-Roger\nVcov Method: Kenward-Roger\nInference:   REML\n\nModel selection criteria:\n     AIC      BIC   logLik deviance \n   817.0    828.5   -402.5    805.0 \n\nCoefficients: \n                     Estimate Std. Error        df t value Pr(&gt;|t|)   \n(Intercept)           0.47589    3.23944  46.14000   0.147  0.88385   \nbasval               -0.30674    0.15200  45.44000  -2.018  0.04951 * \navisitWeek 4         -0.90862    2.39786  47.00000  -0.379  0.70645   \navisitWeek 8        -10.58630    3.45626  47.00000  -3.063  0.00362 **\ntrt2                 -0.34868    1.30287  46.74000  -0.268  0.79016   \ngenderM               2.32931    1.29556  45.99000   1.798  0.07876 . \nbasval:avisitWeek 4  -0.08542    0.11646  47.00000  -0.734  0.46689   \nbasval:avisitWeek 8   0.24779    0.16786  47.00000   1.476  0.14657   \navisitWeek 4:trt2    -0.80100    0.93186  47.00000  -0.860  0.39439   \navisitWeek 8:trt2    -2.20106    1.34318  47.00000  -1.639  0.10795   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCovariance estimate:\n        Week 2  Week 4  Week 8\nWeek 2 18.8295 14.3160 12.0002\nWeek 4 14.3160 21.1623 18.1813\nWeek 8 12.0002 18.1813 28.8384\n\n\nA look into the reference grid shows us the new factor levels for gender. Note that gender itself will not be included in the emmeans() statement, but the output indicates the averaging over its levels (same for the levels of avisit)\n\n# Reference grid shows us the new levels\nemmeans::ref_grid(fit_cat_time2)\n\n'emmGrid' object with variables:\n    basval = 19.56\n    avisit = Week 2, Week 4, Week 8\n    trt = 1, 2\n    gender = F, M\n\n# These two won't yield the same results\nemmeans(fit_cat_time2, ~trt, weights = \"equal\")\n\nNOTE: Results may be misleading due to involvement in interactions\n\n\n trt emmean    SE   df lower.CL upper.CL\n 1    -7.13 0.840 45.8    -8.82    -5.44\n 2    -8.48 0.896 46.4   -10.28    -6.68\n\nResults are averaged over the levels of: avisit, gender \nConfidence level used: 0.95 \n\nemmeans(fit_cat_time2, ~trt*avisit, weights = \"proportional\")\n\n trt avisit emmean    SE   df lower.CL upper.CL\n 1   Week 2  -4.55 0.890 46.2    -6.34    -2.75\n 2   Week 2  -4.89 0.890 46.2    -6.69    -3.10\n 1   Week 4  -7.13 0.941 47.1    -9.02    -5.23\n 2   Week 4  -8.27 0.941 47.1   -10.17    -6.38\n 1   Week 8 -10.29 1.081 46.6   -12.46    -8.11\n 2   Week 8 -12.83 1.081 46.6   -15.01   -10.66\n\nResults are averaged over the levels of: gender \nConfidence level used: 0.95 \n\n\nThe following frequency table shows the imbalance in the distribution of the gender variable. We can see that Treatment 1 has more men than women, whereas Treatment 2 has more women than men.\n\ntable(all2$trt, all2$gender)\n\n   \n     F  M\n  1 30 45\n  2 57 18\n\n\nThe data is no longer balanced across the covariates in the model. The weights = \"equal\" option is agnostic to this imbalance and assigns all levels equal weights, whereas the weights = \"proportional\" assigns a weight reflecting the proportional size of the stratum over which the average is taken.\n\n\n3.4.2 Contrasts"
  },
  {
    "objectID": "s2_inference_continuous.html#fit-diagnostics",
    "href": "s2_inference_continuous.html#fit-diagnostics",
    "title": "3  Inference from Longitudinal Data",
    "section": "3.5 Fit diagnostics",
    "text": "3.5 Fit diagnostics\nThe following section closely follows the content in Chapter 10 in (Fitzmaurice 2011).\nOur analysis should be concluded with a look into the fit diagnostics, more specifically, the residuals. Residuals are defined by the difference between the true responses and the fitted values from the model:\n\\[\nr := y - X\\hat\\beta\\,,\n\\] where \\(\\hat\\beta\\) are the estimated coefficients from our model. Residuals provide an estimate of the true vector of random errors\n\\[\n\\varepsilon = y - X\\beta\\,.\n\\]\nAs per our modeling assumptions, \\(\\varepsilon\\) should follow a normal distribution with mean zero. The mean of the residuals is zero and therefore identical with the mean of the error term. For the covariance of the residuals however, the variance-covariance matrix of \\(\\varepsilon\\) only serves us as an approximation (as suggested by (Fitzmaurice 2011) for all ‘practical applications’):\n\\[\nCov(r) \\approx Cov(\\varepsilon) = R\\,.\n\\] This assumption has several implications on the residual diagnostics:\n\nThe variance is not necessarily constant. Plotting the fitted values versus the residuals might therefore lead to a non-constant range. An examination of the residual variance or autocorrelation among residuals is therefore not very meaningful.\nResiduals from analyses of longitudinal data can exhibit correlation with the covariates. Scatterplots of residuals versus selected covariates can therefore reveal systematic trends (which normally should not be the case).\n\nA transformation of residuals to achieve constant variance and zero correlation is therefore often useful. This transformation uses the so-called Cholesky decomposition of the variance-covariance matrix \\(R\\). Let \\(L\\) be a lower triangular matrix, such that\n\\[\nR = L\\,L'\\,,\n\\] then the transformed residuals are given by \\[\nr^* =  L^{-1}(y - X\\beta)\\,.\n\\] In the mmrm package, transformed residuals can be derived using the type = \"normalized\" option.\nExercise: Which visualisations can you think of that make sense to assess the goodness of fit here? Create a new tibble (or data.frame) containing the variables of importance and try plotting them in a meaningful way. Discuss the results within your group.\nSolution:\nTo avoid repetition, let us first save the important variables to perform fit diagnostics in a tibble.\n\ndf_residuals &lt;- dplyr::tibble(\n  residuals = residuals(fit_cat_time, type = \"normalized\"),\n  predictions = fitted(fit_cat_time),\n  all2\n)\n\nWe can firstly look into a histogram of transformed residuals. The shape should resemble the density function of normal distribution with mean zero and positive variance. Superimposing the density function with mean and SD derived from the model residuals, let’s us see that this is indeed the case. We can also detect a slight skewness to the right.\n\nlibrary(ggplot2)\n\ndf_residuals %&gt;% \n  ggplot(aes(x = residuals)) +\n  geom_histogram(aes(y = after_stat(density)), fill='lightgray', col='black') +\n  stat_function(fun = dnorm, args = list(mean=mean(df_residuals$residuals), sd=sd(df_residuals$residuals)), col='red', lwd=1) +\n  ggtitle(\n    label = \"Histogram of transformed residuals\",\n    subtitle = \"Normal density with mean and SD of residuals superimposed\"\n  )\n\n\n\n\nAlternatively, we can create a Q-Q-Plot of …\n\ndf_residuals %&gt;% \n  ggplot(aes(sample = residuals)) +\n  stat_qq(color = \"blue\") +\n  stat_qq_line() +\n  labs(\n    x = \"Quantiles (Normal distribution)\",\n    y = \"Transformed Residuals\"\n  ) +\n  ggtitle(\n    label = \"Q-Q Plot Transformed Residuals Plot\"\n  )\n\n\n\n\nHow to interprete the Q-Q plot:\nWe can use the following fourfold table to assess the shape characteristics derivable from this plot, depending on where the data on which end of the plot is bend compared to the linear trend line:\n\n\n\n\n\n\n  \n    \n    \n       \n       \n      \n        Upper right corner\n      \n    \n    \n      Above\n      Below\n    \n  \n  \n    Lower left corner\nAbove\nSkewed to the right\nLight-tailed\n    Lower left corner\nBelow\nBelow linear trend line\nSkewed to the left\n  \n  \n  \n\n\n\n\nWe can see that our data is skewed to the right, as the data in the upper right corner and data in the lower left corner of the plot bend above the linear trend line. This is also a trend we can observe from the histogram.\n\ndf_residuals %&gt;% \n  ggplot(aes(x = predictions, y = residuals)) +\n  geom_point() +\n  geom_smooth(method = lm, color = \"blue\") +\n  geom_hline(yintercept = 0, show.legend = FALSE, linetype = 2) +\n  ggtitle(\n    label = \"Residual Plot of predicted values vs. transformed residuals\"\n  )\n\n\n\n\nWhat do we see?\n\nThe points in the plot look well dispersed and symmetric around zero. The fitted line shows no departure from zero.\nThere is no systematic trend, but a rather random scatter.\n\n\ndf_residuals %&gt;% \n  ggplot(aes(x = predictions, y = change)) +\n  geom_point() +\n  geom_smooth(method = lm, color = \"blue\")\n\n\n\n\n\n3.5.1 Addendum on RS&I Models\nDifferent dosing/ assessment frequency between treatment arms in parallel design -&gt; oncology (chemo with fixed cycles vs immune-therapy)\n\n\n\n\nFitzmaurice, Laird, G. M. 2011. Applied Longitudinal Analysis. Vol. 2. USA: New York, Wiley. https://doi.org/10.1002/9781119513469."
  },
  {
    "objectID": "s3_missingness.html#missing-data-mechanisms",
    "href": "s3_missingness.html#missing-data-mechanisms",
    "title": "4  Missing Data",
    "section": "4.1 Missing Data Mechanisms",
    "text": "4.1 Missing Data Mechanisms\nTo understand the nature of missing data in our clinical trial, we consider the following taxonomy, introduced by (Roderick JA Little 2019). We differentiate between the following three types of missing data:\n\nMissing Completely at Random (MCAR): Conditional on all covariates in our analysis, the probability of missingness does not depend on either observed or unobserved values of the response variable.\nMissing at Random (MAR): Conditional on all covariates and observed response values in our analysis, the probability of missingness does not depend on the unobserved values of the response variable.\nMissing not at Random (MNAR): Conditional on all covariates and observed response values in our analysis, the probability of missingness does depend on the unobserved values of the response variable.\n\n(Craig Mallinckrodt 2016) give the following interpretation around the three types of missingness:\n“With MCAR, the outcome variable is not related to the probability of dropout (after taking into account covariates). In MAR, the observed values of the outcome variable are related to the probability of dropout, but the unobserved outcomes are not (after taking into account covariates and observed outcomes). In MNAR the unobserved outcomes are related to the probability of dropout even after the observed outcomes and covariates have been taken into account.”\nThe following two sections outline handling strategies for missing data. However, the best approach to handle missing data is to minimise its extent. While the occurence of missing data can rarely be avoided at all (think about the collection of questionnaire data in oncology studies and the missing data after subjects die), it is important to pursue an “as complete as can be” data collection.\nBaseline and screening data are of utmost importance in a pursuit of data completeness. If a screening value is missing, but was meant to be used as a covariate, this subjects’ whole data will be dropped from the analysis even if all responses were observed. If the baseline response variable was missing we are unable to compute a change from baseline, which also leads to the loss of this subjects’ data in the model (although LDA models are still able to provide an estimate) even if all post-baseline values were observed."
  },
  {
    "objectID": "s3_missingness.html#missing-data-handling-i-descriptive-stats-visualisations",
    "href": "s3_missingness.html#missing-data-handling-i-descriptive-stats-visualisations",
    "title": "4  Missing Data",
    "section": "4.2 Missing data handling I (descriptive stats + visualisations)",
    "text": "4.2 Missing data handling I (descriptive stats + visualisations)"
  },
  {
    "objectID": "s3_missingness.html#missing-data-handling-ii-analytic-approaches",
    "href": "s3_missingness.html#missing-data-handling-ii-analytic-approaches",
    "title": "4  Missing Data",
    "section": "4.3 Missing data handling II (analytic approaches)",
    "text": "4.3 Missing data handling II (analytic approaches)\n\n4.3.1 Complete Case Analyses\n\n\n\n\nCraig Mallinckrodt, Ilya Lipkovich. 2016. Analyzing Longitudinal Clinical Trial Data: A Practical Guide. Vol. 1. USA: Chapman; Hall/CRC. https://doi.org/10.1201/9781315186634.\n\n\nRoderick JA Little, Donald B. Rubin. 2019. Statistical Analysis with Missing Data. Vol. 3. USA: New York, Wiley. https://doi.org/10.1002/9781119482260."
  },
  {
    "objectID": "s4_sensitivity_analyses.html#purpose-of-sensitivity-analyses",
    "href": "s4_sensitivity_analyses.html#purpose-of-sensitivity-analyses",
    "title": "5  Sensitivity Analyses",
    "section": "5.1 Purpose of sensitivity analyses",
    "text": "5.1 Purpose of sensitivity analyses\n\nConsider sensivitiy analyses to check model assumptions e.g. assumption of MAR.\nComparing results from sensitivity analyses: how much inference rely on the assumptions.\nHere, inference with regard to the treatment effect. Thus, investigate how treatment effects vary depending on assumptions (about missing data).\nUncertainty from incompleteness cannot be objectively evaluated from observed data so there is a need for missing data sensitivity analyses."
  },
  {
    "objectID": "s4_sensitivity_analyses.html#mmrm-vs.-mi",
    "href": "s4_sensitivity_analyses.html#mmrm-vs.-mi",
    "title": "5  Sensitivity Analyses",
    "section": "5.2 MMRM vs. MI",
    "text": "5.2 MMRM vs. MI\n\nFlexibility in modeling treatment effects over time and the within-patient error correlation structure makes MMRM a widely useful analysis.\nMMRM, MI: two major approaches to missing data with good statistical properties. Both rely on MAR assumption (for MI: standard implementation).\nMMRM: missing values implicitly imputed, MI: missing values explicitly imputed.\nMMRM vs. MI: approximately equivalent provided the variables used in the imputation model are the same as those included in the analysis model (level of equivalence will depend on the number of imputations)\nMI: imputation model with at least those variables from the primary model, additional auxiliary variables can be used in the imputation model to improve the accuracy of the missing data prediction.\nHandling missing not at random (MNAR) possible for MI (e.g. reference-based imputation) but not within MMRM.\nMMRM does not work if missing baseline values are present. Missing baseline values can be imputed first. Additionally, at least one post-baseline value has to be observed. Alternative: LDA where baseline is part of the response vector.\n\nNote that, when implemented in similar manners, MI and MMRM have similar assumptions and yield similar results. Thus, MI implemented similarly to MMRM is not a sensitivity analysis!"
  },
  {
    "objectID": "s4_sensitivity_analyses.html#missing-covariates-baseline-data-only",
    "href": "s4_sensitivity_analyses.html#missing-covariates-baseline-data-only",
    "title": "5  Sensitivity Analyses",
    "section": "5.3 Missing covariates (baseline data) only",
    "text": "5.3 Missing covariates (baseline data) only\n\nMissing baseline value of the outcome (and other covariates) is a common situation\nMMRM not efficient or potential biased estimates as subjects with missing covariates are excluded from the analysis\n(Kayembe and Breukelen 2022) compared different methods e.g. unadjusted analysis, complete case, mean imputation, MI: mean imputation seems to be appropriate as long as the covariates are measured before randomization (produces unbiased treatment effect estimates with good coverage, easy to implement)\n\nNow, we consider the situation as in our data sets: baseline observed, no intermittent missing values, drop-outs = monotone missing pattern"
  },
  {
    "objectID": "s4_sensitivity_analyses.html#sensitivity-analyses---simple-approaches",
    "href": "s4_sensitivity_analyses.html#sensitivity-analyses---simple-approaches",
    "title": "5  Sensitivity Analyses",
    "section": "5.4 Sensitivity analyses - Simple approaches",
    "text": "5.4 Sensitivity analyses - Simple approaches\nIn general, these simple approaches are not recommended for use. Methods are of historic interest and provide a useful starting point. Here, we consider two simple approaches. We will apply these two methods in the practical part to compare results.\n\n5.4.1 Last observation carried forward (LOCF)\nLOCF imputes all missing values for each subject using the last observed value for that subject. Typically, under LOCF the repeated masures nature of the data is ignored and a single outcome for each subject is analyzed. LOCF was used in the past, justified as it was thought that it provides conservative estimates. However, conditions under which LOCF yield conservative estimates and maintain control of Type I error rates are not straightforward and cannot be assured at the beginning of the trial. For example, LOCF is likely to overestimate treatment benefit if dop-out in the control gorup is more frequent.\n\n\n5.4.2 Complete case (CC)\nOther names: observed case/ completers analysis Reduce the data set selecting only those subjects with observed outcome value(s). Completers analysis may create selection bias, may cause overestimation of within group effects particularly at the last scheduled visit."
  },
  {
    "objectID": "s4_sensitivity_analyses.html#sensitivity-analyses---handling-nonignorable-missingness-mnar",
    "href": "s4_sensitivity_analyses.html#sensitivity-analyses---handling-nonignorable-missingness-mnar",
    "title": "5  Sensitivity Analyses",
    "section": "5.5 Sensitivity analyses - Handling nonignorable missingness (MNAR)",
    "text": "5.5 Sensitivity analyses - Handling nonignorable missingness (MNAR)\n\nAssumption of MAR is often reasonable, but possibility of data missing not at random (MNAR) is difficult to rule out.\nThus, analysis under MNAR needed.\nAnalysis under MNAR: these methods are heavily assumption driven and the assumptions are not testable as we do not have the missing data.\nConsider a sensitivity analysis framework allowing assessment of robustness of results to the various assumptions.\nMNAR methods: different possibilities e.g. class of pattern-mixture models. The pattern-mixture model allows missing outcomes to be imputed under a chosen scenario and in this way can be used to complete the data set and apply the primary analysis to this completed data set.\nMI can be used to explore departures from MAR (for analysis under a MNAR assumption). This is referred to as controlled MI and includes delta-based MI and reference-based MI (belong to the class of pattern mixture models). Data is imputed under an alternative MNAR distribution that reflects a relevant scenario for the unobserved data. The imputed data sets are then analysed as with standard MI.\n\n\n5.5.1 Reference-based multiple imputation\n\nHas recieved increasing attention in clinical trials as it provides an attractive approach for a sensitivity analysis because missing data assumptions are framed in an intuitive way. The departure from MAR is captured in a qualitative way, making the formulation of the problem intuitive.\nFor example, a plausible MNAR mechanism in a placebo‐controlled trial is to assume that subjects in the experimental arm who dropped out stop taking their treatment and have similar outcomes to those in the placebo arm.\nRemember: MI under MAR assumes that the outcome distribution of patients with missing data is the same as the outcome distribution of patients with complete data, conditional on relevant covariates. However, if most patients withdraw from the study after treatment discontinuation, then this is not plausible, as patients who withdraw from the study treatment are expected to have a worse otucome than patients who stay on study treatment. Thus, addressing missing data under a MAR assumption estimates a hypothetical estimand and not a treatment policy estimand.\nDifferent options to handle missing outcome data for reference-based imputation were described (Carpenter and Kenward 2013): e.g. jump to reference (J2R), copy reference (CR), copy increments in reference (CIR)\n\nJump to reference J2R assumes that after treatment discontinuation, the patient’s mean outcome distribution is that of a reference group, usually the control group. This is a very extreme assumption, as this implies that any efficacy of the drug vanishes immediately after discontinuation - may be plausible for symptomatic treatments.\nCopy reference CR assumes that the patient’s outcome distribution both before and after treatment discontinuation is the same as the distribution of the reference group. This has a milder effect than J2R: If a treatment-group patient has an outcome that is better than the reference group mean before treatment discontinuation, their imputed values after treatment discontinuation will also be better than the reference group mean.\nCopy increments in reference CIR assumes that after treatment discontinuation, the increments are the same as those from the reference group. This is much milder than J2R and CR and implies that benefit gained from the treatment before discontinuation is not lost.\nThe conventional approach to analyse data using these reference based approaches is MI, following the same steps as MI under MAR.\nSoftware, R: the rbmi package supports reference-based strategies (Gower-Page and Wolbers 2022)\n\n\n5.5.2 Delta-based multiple imputation\n\nImpute data assuming all unobserved subjects having a poorer or better response than those observed, by adding or subtracting a delta parameter \\(\\delta\\) to the expected value of the e.g. MAR imputed values.\nDelta can be implemented in all treatment groups, or in only one group, or may vary by treatment group or an alternative specified factor.\nChoice of values for the sensitivity parameter \\(\\delta\\): e.g. selection by content experts.\nSteps: 1. missing values are imputed using standard MI procedure e.g. under MAR (but can also be under MNAR e.g. combinded with copy reference approach), 2. imputed values are shifted by adding some fixed value \\(\\delta\\) to reflect the MNAR mechanism, 3. analysis with standard statistical methods including Rubin’s rule to combine results"
  },
  {
    "objectID": "s4_sensitivity_analyses.html#practical-part",
    "href": "s4_sensitivity_analyses.html#practical-part",
    "title": "5  Sensitivity Analyses",
    "section": "5.6 Practical part",
    "text": "5.6 Practical part\n\nTake the (all2) high2 data set\nLook at the MMRM and at the complete case (CC) analysis (refer to section missingness for the all2 data set).\nApply additionally LOCF and compare results.\nTry MNAR method reference-based MI with J2R by using the rbmi package. Compare with the other results.\n\n\n5.6.1 Set-up to use rbmi\nHave a short look at the rbmi() package first.\n\nlibrary(rbmi)\n?rbmi\n\nvignette(topic = \"quickstart\", package = \"rbmi\")\n\nstarting httpd help server ... done\n\n\nThe workflow is based on 4 core functions: - draws() - fits the imputation models, different methods possible, we will use method_bayes() for MI based on Bayesian posterior parameter draws from MCMC sampling - impute() - creates multiple imputed data sets - analyse() - analyses each of the multiple imputed data sets, default = ancova, other options possible - pool() - combines the results across imputed data sets, for method_bayes (see above) inference is based on Rubin’s rule\nImplemented imputation strategies in rbmi: - Missing at Random (MAR) - Jump to Reference (JR) - Copy Reference (CR) - Copy Increments in Reference (CIR)\nI will show how it looks like for the all2 data set and you will then explore the methods using the high2 data set.\n\n\n5.6.2 Plenum - Solution for all2 data set\n\nComplete case\n\n\nall2.cc &lt;- all2 %&gt;% dplyr::filter(dropout_grp==\"Completer\")\n\nfit_cc &lt;- mmrm::mmrm(\n  formula = chgdrop ~ basval*avisit + trt*avisit + us(avisit | subject),\n  data = all2.cc,\n  control = mmrm_control(method = \"Kenward-Roger\")\n)\n\nsummary(fit_cc)\n\nmmrm fit\n\nFormula:     chgdrop ~ basval * avisit + trt * avisit + us(avisit | subject)\nData:        all2.cc (used 111 observations from 37 subjects with maximum 3 \ntimepoints)\nCovariance:  unstructured (6 variance parameters)\nMethod:      Kenward-Roger\nVcov Method: Kenward-Roger\nInference:   REML\n\nModel selection criteria:\n     AIC      BIC   logLik deviance \n   608.8    618.5   -298.4    596.8 \n\nCoefficients: \n                     Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)           1.89223    3.60558  33.99000   0.525 0.603124    \nbasval               -0.31950    0.17281  33.99000  -1.849 0.073201 .  \navisitWeek 4         -1.63943    2.46046  34.00000  -0.666 0.509708    \navisitWeek 8        -12.36928    3.39084  34.00000  -3.648 0.000877 ***\ntrt2                 -1.13978    1.56623  33.99000  -0.728 0.471768    \nbasval:avisitWeek 4  -0.05179    0.11793  34.00000  -0.439 0.663301    \nbasval:avisitWeek 8   0.33515    0.16252  34.00000   2.062 0.046899 *  \navisitWeek 4:trt2    -0.99990    1.06880  34.00000  -0.936 0.356113    \navisitWeek 8:trt2    -1.78825    1.47295  34.00000  -1.214 0.233089    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCovariance estimate:\n        Week 2  Week 4  Week 8\nWeek 2 23.2319 16.8721 14.6422\nWeek 4 16.8721 21.7589 17.9166\nWeek 8 14.6422 17.9166 27.5347\n\nmodel_lsmeans &lt;- emmeans::emmeans(fit_cc, ~trt*avisit, weights = \"proportional\")\nmodel_lsmeans\n\n trt avisit emmean   SE df lower.CL upper.CL\n 1   Week 2  -4.33 1.12 34    -6.61    -2.06\n 2   Week 2  -5.47 1.09 34    -7.69    -3.26\n 1   Week 4  -6.98 1.09 34    -9.19    -4.77\n 2   Week 4  -9.12 1.06 34   -11.27    -6.97\n 1   Week 8 -10.17 1.21 34   -12.63    -7.71\n 2   Week 8 -13.10 1.18 34   -15.49   -10.71\n\nConfidence level used: 0.95 \n\n\n\nMMRM\n\n\nfit_drop &lt;- mmrm::mmrm(\n  formula = chgdrop ~ basval*avisit + trt*avisit + us(avisit | subject),\n  data = all2,\n  control = mmrm_control(method = \"Kenward-Roger\")\n)\n\nsummary(fit_drop)\n\nmmrm fit\n\nFormula:     chgdrop ~ basval * avisit + trt * avisit + us(avisit | subject)\nData:        all2 (used 129 observations from 50 subjects with maximum 3 \ntimepoints)\nCovariance:  unstructured (6 variance parameters)\nMethod:      Kenward-Roger\nVcov Method: Kenward-Roger\nInference:   REML\n\nModel selection criteria:\n     AIC      BIC   logLik deviance \n   709.2    720.7   -348.6    697.2 \n\nCoefficients: \n                     Estimate Std. Error        df t value Pr(&gt;|t|)   \n(Intercept)           1.98452    3.27498  46.99000   0.606  0.54745   \nbasval               -0.31235    0.15906  46.99000  -1.964  0.05549 . \navisitWeek 4         -0.90712    2.44043  39.90000  -0.372  0.71208   \navisitWeek 8        -11.82291    3.34959  36.17000  -3.530  0.00115 **\ntrt2                 -1.18993    1.27272  46.99000  -0.935  0.35460   \nbasval:avisitWeek 4  -0.07256    0.11799  39.85000  -0.615  0.54206   \nbasval:avisitWeek 8   0.31809    0.16104  35.86000   1.975  0.05598 . \navisitWeek 4:trt2    -0.90513    1.00404  40.53000  -0.901  0.37266   \navisitWeek 8:trt2    -1.70761    1.42670  38.06000  -1.197  0.23875   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCovariance estimate:\n        Week 2  Week 4  Week 8\nWeek 2 20.6136 15.5273 13.4180\nWeek 4 15.5273 21.6600 17.7420\nWeek 8 13.4180 17.7420 27.3112\n\nmodel_lsmeans &lt;- emmeans::emmeans(fit_drop, ~trt*avisit, weights = \"proportional\")\nmodel_lsmeans\n\n trt avisit emmean    SE   df lower.CL upper.CL\n 1   Week 2  -4.10 0.900 47.0    -5.91    -2.29\n 2   Week 2  -5.29 0.899 47.0    -7.10    -3.48\n 1   Week 4  -6.42 0.974 46.5    -8.38    -4.46\n 2   Week 4  -8.52 0.951 44.8   -10.43    -6.60\n 1   Week 8  -9.73 1.142 40.4   -12.03    -7.42\n 2   Week 8 -12.62 1.114 40.1   -14.88   -10.37\n\nConfidence level used: 0.95 \n\n\n\nLOCF\n\n\nall2 &lt;- all2 %&gt;% \n  dplyr::group_by(subject) %&gt;% \n  dplyr::mutate( drop=max(week) )\n\nall2.locf&lt;-all2 %&gt;% dplyr::filter(week==drop)\n\nancova &lt;- aov(change ~ basval + trt, data = all2.locf)\nsummary(ancova)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \nbasval       1    1.2    1.23   0.044 0.8340  \ntrt          1  143.2  143.22   5.186 0.0274 *\nResiduals   47 1297.9   27.61                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nancova$coefficients\n\n(Intercept)      basval        trt2 \n-8.60177970 -0.06455658 -3.39098716 \n\nall2.locf %&gt;% ungroup() %&gt;%\n  select(change, group) %&gt;%\n               tbl_summary(by = group,\n                           statistic = list(\n                             all_continuous() ~ \"{mean} ({sd})\"), \n                           digits = all_continuous() ~ 2 ) \n\n\n\n\n\n  \n    \n    \n      Characteristic\n      Arm 1, N = 251\n      Arm 2, N = 251\n    \n  \n  \n    change\n-9.88 (4.85)\n-13.24 (5.54)\n  \n  \n  \n    \n      1 Mean (SD)\n    \n  \n\n\n\n\n\nJ2R\n\n\n# Define the names of key variables in the data set\nset_mi&lt;-set_vars(\n  subjid = \"subject\",\n  visit = \"avisit\",\n  outcome = \"chgdrop\",\n  group = \"group\",\n  covariates = c(\"basval * avisit\", \"group * avisit\")\n)\n\nvars_an&lt;-set_mi\nvars_an$covariates &lt;- \"basval\"\n\n# Define the imputation strategy for each subject with at least one missing observation\ndat_ice &lt;- all2 %&gt;% \n  arrange(subject, avisit) %&gt;% \n  filter(is.na(chgdrop)) %&gt;% \n  group_by(subject) %&gt;% \n  slice(1) %&gt;%\n  ungroup() %&gt;% \n  select(subject, avisit) %&gt;% \n  mutate(strategy = \"JR\")\n\n# Define the imputation method\nmethod &lt;- method_bayes(\n  burn_in = 200,\n  burn_between = 5,\n  n_samples = 100,\n  seed = 072407\n)\n\ndraw_all2&lt;-draws(data=all2, data_ice = dat_ice, vars=set_mi, method=method, ncores = 1, quiet = FALSE)\n\n\nSAMPLING FOR MODEL 'MMRM' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.000119 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.19 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:   1 / 700 [  0%]  (Warmup)\nChain 1: Iteration:  70 / 700 [ 10%]  (Warmup)\nChain 1: Iteration: 140 / 700 [ 20%]  (Warmup)\nChain 1: Iteration: 201 / 700 [ 28%]  (Sampling)\nChain 1: Iteration: 270 / 700 [ 38%]  (Sampling)\nChain 1: Iteration: 340 / 700 [ 48%]  (Sampling)\nChain 1: Iteration: 410 / 700 [ 58%]  (Sampling)\nChain 1: Iteration: 480 / 700 [ 68%]  (Sampling)\nChain 1: Iteration: 550 / 700 [ 78%]  (Sampling)\nChain 1: Iteration: 620 / 700 [ 88%]  (Sampling)\nChain 1: Iteration: 690 / 700 [ 98%]  (Sampling)\nChain 1: Iteration: 700 / 700 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.14 seconds (Warm-up)\nChain 1:                0.224 seconds (Sampling)\nChain 1:                0.364 seconds (Total)\nChain 1: \n\nimputeObj &lt;- impute(\n  draw_all2,\n  references = c(\"Arm 1\" = \"Arm 1\", \"Arm 2\" = \"Arm 1\")\n)\n\nimputed_all2 &lt;- extract_imputed_dfs(imputeObj)\n\nanaObj &lt;- analyse(\n  imputeObj,\n  vars = vars_an\n)\n\npoolObj &lt;- pool(anaObj)\nas.data.frame(poolObj)\n\n       parameter        est        se        lci        uci         pval\n1     trt_Week 2  -1.189928 1.2864325  -3.780746  1.4008900 3.598958e-01\n2 lsm_ref_Week 2  -4.125036 0.9088264  -5.955372 -2.2947002 4.178855e-05\n3 lsm_alt_Week 2  -5.314964 0.9088264  -7.145300 -3.4846279 5.199440e-07\n4     trt_Week 4  -1.920738 1.3711052  -4.689028  0.8475529 1.687137e-01\n5 lsm_ref_Week 4  -6.404449 0.9777849  -8.379818 -4.4290798 7.521493e-08\n6 lsm_alt_Week 4  -8.325187 0.9654627 -10.274069 -6.3763039 8.265733e-11\n7     trt_Week 8  -2.211225 1.6967275  -5.649685  1.2272346 2.005871e-01\n8 lsm_ref_Week 8  -9.656881 1.2244745 -12.142348 -7.1714149 2.770935e-09\n9 lsm_alt_Week 8 -11.868107 1.1717638 -14.238826 -9.4973871 1.948658e-12\n\n\n\nChange from J2R to CIR Use the additional argument update_strategies in the impute function.\n\n\ndat_ice_CIR &lt;- dat_ice %&gt;% \n  mutate(strategy = ifelse(strategy == \"JR\", \"CIR\", strategy))\n\nimputeObj_CIR &lt;- impute(\n  draw_all2,\n  references = c(\"Arm 1\" = \"Arm 1\", \"Arm 2\" = \"Arm 1\"),\n  update_strategy = dat_ice_CIR\n)\n\nanaObj_CIR &lt;- analyse(\n  imputeObj_CIR,\n  vars = vars_an\n)\n\npoolObj_CIR &lt;- pool(anaObj_CIR)\nas.data.frame(poolObj_CIR)\n\n       parameter        est        se        lci        uci         pval\n1     trt_Week 2  -1.189928 1.2864325  -3.780746  1.4008900 3.598958e-01\n2 lsm_ref_Week 2  -4.125036 0.9088264  -5.955372 -2.2947002 4.178855e-05\n3 lsm_alt_Week 2  -5.314964 0.9088264  -7.145300 -3.4846279 5.199440e-07\n4     trt_Week 4  -2.014793 1.3710976  -4.782582  0.7529964 1.492281e-01\n5 lsm_ref_Week 4  -6.389437 0.9827301  -8.375095 -4.4037784 8.988973e-08\n6 lsm_alt_Week 4  -8.404230 0.9687489 -10.359825 -6.4486346 7.096096e-11\n7     trt_Week 8  -2.609022 1.6166710  -5.879658  0.6616141 1.146759e-01\n8 lsm_ref_Week 8  -9.646948 1.1739906 -12.026748 -7.2671472 8.012076e-10\n9 lsm_alt_Week 8 -12.255969 1.1568184 -14.598416 -9.9135228 7.351873e-13\n\n\n\n\n5.6.3 Solution for high2 data set\nFirst, fill in missing visits. This was not necessary in the all2 data set. Note, change is the outcome variable and not chgdrop as in all2\n\nhigh2 &lt;- high2 %&gt;% ungroup()\n\nhigh2_expand &lt;- expand_locf(\n  high2,\n  subject = levels(high2$subject),  \n  avisit = levels(high2$avisit),\n  vars = c(\"basval\",\"trt\",\"group\"),\n  group = c(\"subject\"),\n  order = c(\"subject\", \"avisit\")\n)\n\n\nMMRM\n\n\nfit_mmrm &lt;- mmrm::mmrm(\n  formula = change ~ basval*avisit + trt*avisit + us(avisit | subject),\n  data = high2,\n  control = mmrm_control(method = \"Kenward-Roger\")\n)\n\nsummary(fit_mmrm)\n\nmmrm fit\n\nFormula:     change ~ basval * avisit + trt * avisit + us(avisit | subject)\nData:        high2 (used 830 observations from 200 subjects with maximum 5 \ntimepoints)\nCovariance:  unstructured (15 variance parameters)\nMethod:      Kenward-Roger\nVcov Method: Kenward-Roger\nInference:   REML\n\nModel selection criteria:\n     AIC      BIC   logLik deviance \n  4779.1   4828.6  -2374.6   4749.1 \n\nCoefficients: \n                     Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)           3.33421    1.12651 196.97000   2.960  0.00346 ** \nbasval               -0.27934    0.05962 196.97000  -4.685  5.2e-06 ***\navisitWeek 2         -0.15400    1.17265 181.53000  -0.131  0.89566    \navisitWeek 4         -1.00849    1.35934 172.12000  -0.742  0.45916    \navisitWeek 6         -3.27037    1.53582 166.05000  -2.129  0.03470 *  \navisitWeek 8         -3.93835    1.65523 140.95000  -2.379  0.01868 *  \ntrt2                 -0.04273    0.64969 196.97000  -0.066  0.94763    \nbasval:avisitWeek 2  -0.08292    0.06254 181.91000  -1.326  0.18659    \nbasval:avisitWeek 4  -0.10700    0.07290 173.67000  -1.468  0.14396    \nbasval:avisitWeek 6  -0.01321    0.08198 165.55000  -0.161  0.87216    \nbasval:avisitWeek 8   0.01778    0.08902 143.32000   0.200  0.84197    \navisitWeek 2:trt2    -0.61015    0.69414 181.41000  -0.879  0.38057    \navisitWeek 4:trt2    -1.41851    0.81728 175.52000  -1.736  0.08438 .  \navisitWeek 6:trt2    -2.31835    0.91503 165.19000  -2.534  0.01222 *  \navisitWeek 8:trt2    -2.47738    0.99465 143.57000  -2.491  0.01389 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCovariance estimate:\n        Week 1  Week 2  Week 4  Week 6  Week 8\nWeek 1 20.9961 17.1332 15.4142 15.3503 15.8717\nWeek 2 17.1332 35.2157 25.8380 25.5499 24.3926\nWeek 4 15.4142 25.8380 38.8771 33.0523 30.1128\nWeek 6 15.3503 25.5499 33.0523 43.7638 39.3236\nWeek 8 15.8717 24.3926 30.1128 39.3236 47.7371\n\nmodel_lsmeans &lt;- emmeans::emmeans(fit_mmrm, ~trt*avisit, weights = \"proportional\")\nmodel_lsmeans\n\n trt avisit emmean    SE  df lower.CL upper.CL\n 1   Week 1  -1.61 0.458 197    -2.52   -0.711\n 2   Week 1  -1.66 0.459 197    -2.56   -0.752\n 1   Week 2  -3.24 0.609 191    -4.44   -2.036\n 2   Week 2  -3.89 0.613 193    -5.10   -2.681\n 1   Week 4  -4.52 0.656 182    -5.81   -3.223\n 2   Week 4  -5.98 0.656 182    -7.27   -4.684\n 1   Week 6  -5.12 0.718 168    -6.53   -3.701\n 2   Week 6  -7.48 0.715 166    -8.89   -6.067\n 1   Week 8  -5.24 0.785 149    -6.79   -3.686\n 2   Week 8  -7.76 0.762 139    -9.26   -6.251\n\nConfidence level used: 0.95 \n\n\n\nComplete case\n\n\nhigh2.cc&lt;- high2 %&gt;% dplyr::filter(drop==8)\n\nfit_cc &lt;- mmrm::mmrm(\n  formula = change ~ basval*avisit + trt*avisit + us(avisit | subject),\n  data = high2.cc,\n  control = mmrm_control(method = \"Kenward-Roger\")\n)\n\nsummary(fit_cc)\n\nmmrm fit\n\nFormula:     change ~ basval * avisit + trt * avisit + us(avisit | subject)\nData:        high2.cc (used 649 observations from 130 subjects with maximum 5 \ntimepoints)\nCovariance:  unstructured (15 variance parameters)\nMethod:      Kenward-Roger\nVcov Method: Kenward-Roger\nInference:   REML\n\nModel selection criteria:\n     AIC      BIC   logLik deviance \n  3693.8   3736.9  -1831.9   3663.8 \n\nCoefficients: \n                     Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)           3.26341    1.33919 127.00000   2.437   0.0162 *  \nbasval               -0.29475    0.07287 127.00000  -4.045 9.03e-05 ***\navisitWeek 2         -0.11631    1.40264 127.74000  -0.083   0.9340    \navisitWeek 4         -0.77525    1.56814 127.01000  -0.494   0.6219    \navisitWeek 6         -3.27487    1.59347 127.01000  -2.055   0.0419 *  \navisitWeek 8         -3.94403    1.69295 127.01000  -2.330   0.0214 *  \ntrt2                 -0.06433    0.81571 127.00000  -0.079   0.9373    \nbasval:avisitWeek 2  -0.11719    0.07647 128.03000  -1.532   0.1279    \nbasval:avisitWeek 4  -0.18029    0.08533 127.01000  -2.113   0.0366 *  \nbasval:avisitWeek 6  -0.10859    0.08671 127.01000  -1.252   0.2127    \nbasval:avisitWeek 8  -0.06299    0.09212 127.01000  -0.684   0.4954    \navisitWeek 2:trt2    -0.32364    0.85100 127.17000  -0.380   0.7043    \navisitWeek 4:trt2    -1.07631    0.95516 127.01000  -1.127   0.2619    \navisitWeek 6:trt2    -1.35403    0.97059 127.01000  -1.395   0.1654    \navisitWeek 8:trt2    -1.65323    1.03118 127.01000  -1.603   0.1114    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCovariance estimate:\n        Week 1  Week 2  Week 4  Week 6  Week 8\nWeek 1 21.0337 16.0819 13.3827 12.2436 13.2340\nWeek 2 16.0819 34.0261 22.4388 20.5555 20.0298\nWeek 4 13.3827 22.4388 34.9214 27.3814 25.0059\nWeek 6 12.2436 20.5555 27.3814 33.6239 30.2604\nWeek 8 13.2340 20.0298 25.0059 30.2604 39.6554\n\nmodel_lsmeans &lt;- emmeans::emmeans(fit_cc, ~trt*avisit, weights = \"proportional\")\nmodel_lsmeans\n\n trt avisit emmean    SE  df lower.CL upper.CL\n 1   Week 1  -1.91 0.595 127    -3.08   -0.731\n 2   Week 1  -1.97 0.550 127    -3.06   -0.885\n 1   Week 2  -4.08 0.756 126    -5.58   -2.585\n 2   Week 2  -4.47 0.702 127    -5.86   -3.080\n 1   Week 4  -5.85 0.764 127    -7.36   -4.336\n 2   Week 4  -6.99 0.706 127    -8.38   -5.591\n 1   Week 6  -7.09 0.749 127    -8.57   -5.607\n 2   Week 6  -8.51 0.692 127    -9.88   -7.138\n 1   Week 8  -6.96 0.811 127    -8.56   -5.352\n 2   Week 8  -8.67 0.750 127   -10.16   -7.191\n\nConfidence level used: 0.95 \n\n\n\nLOCF\n\n\nhigh2.locf&lt;-high2 %&gt;% dplyr::filter(week==drop)\n\nancova &lt;- aov(change ~ basval + trt, data = high2.locf)\nsummary(ancova)\n\n             Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nbasval        1    483   483.3   9.709 0.00211 **\ntrt           1    241   241.4   4.851 0.02880 * \nResiduals   197   9805    49.8                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nancova$coefficients\n\n(Intercept)      basval        trt2 \n  0.3536399  -0.2648315  -2.2086854 \n\nhigh2.locf %&gt;% ungroup() %&gt;%\n  select(change, group) %&gt;%\n  tbl_summary(by = group,\n              statistic = list(\n                all_continuous() ~ \"{mean} ({sd})\"), \n              digits = all_continuous() ~ 2 )\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      Arm 1, N = 1001\n      Arm 2, N = 1001\n    \n  \n  \n    change\n-4.22 (6.38)\n-6.72 (7.90)\n  \n  \n  \n    \n      1 Mean (SD)\n    \n  \n\n\n\n\n\nJ2R\n\n\nset_mi&lt;-set_vars(\n  subjid = \"subject\",\n  visit = \"avisit\",\n  outcome = \"change\",\n  group = \"group\",\n  covariates = c(\"basval * avisit\", \"group * avisit\")\n)\n\nvars_an&lt;-set_mi\nvars_an$covariates &lt;- \"basval\"\n\ndat_ice &lt;- high2_expand %&gt;% \n  arrange(subject, avisit) %&gt;% \n  filter(is.na(change)) %&gt;% \n  group_by(subject) %&gt;% \n  slice(1) %&gt;%\n  ungroup() %&gt;% \n  select(subject, avisit) %&gt;% \n  mutate(strategy = \"JR\")\n\nmethod &lt;- method_bayes(\n  burn_in = 200,\n  burn_between = 5,\n  n_samples = 100,\n  seed = 072407\n)\n\ndraw_high2&lt;-draws(data=high2_expand, data_ice = dat_ice, vars=set_mi, method=method, ncores = 1, quiet = FALSE)\n\n\nSAMPLING FOR MODEL 'MMRM' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.000245 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 2.45 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:   1 / 700 [  0%]  (Warmup)\nChain 1: Iteration:  70 / 700 [ 10%]  (Warmup)\nChain 1: Iteration: 140 / 700 [ 20%]  (Warmup)\nChain 1: Iteration: 201 / 700 [ 28%]  (Sampling)\nChain 1: Iteration: 270 / 700 [ 38%]  (Sampling)\nChain 1: Iteration: 340 / 700 [ 48%]  (Sampling)\nChain 1: Iteration: 410 / 700 [ 58%]  (Sampling)\nChain 1: Iteration: 480 / 700 [ 68%]  (Sampling)\nChain 1: Iteration: 550 / 700 [ 78%]  (Sampling)\nChain 1: Iteration: 620 / 700 [ 88%]  (Sampling)\nChain 1: Iteration: 690 / 700 [ 98%]  (Sampling)\nChain 1: Iteration: 700 / 700 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.861 seconds (Warm-up)\nChain 1:                1.558 seconds (Sampling)\nChain 1:                2.419 seconds (Total)\nChain 1: \n\nimputeObj &lt;- impute(\n  draw_high2,\n  references = c(\"Arm 1\" = \"Arm 1\", \"Arm 2\" = \"Arm 1\")\n)\n\nimputed_high2 &lt;- extract_imputed_dfs(imputeObj)\n\nanaObj &lt;- analyse(\n  imputeObj,\n  vars = vars_an\n)\n\n#For method_bayes() or method_approxbayes() pooling and inference are based on Rubin’s rules.\npoolObj &lt;- pool(anaObj)\nas.data.frame(poolObj)\n\n        parameter         est        se       lci        uci         pval\n1      trt_Week 1 -0.04272539 0.6513099 -1.327240  1.2417894 9.477641e-01\n2  lsm_ref_Week 1 -1.64363730 0.4593710 -2.549610 -0.7376649 4.367614e-04\n3  lsm_alt_Week 1 -1.68636270 0.4593710 -2.592335 -0.7803903 3.118171e-04\n4      trt_Week 2 -0.56805913 0.8694895 -2.283587  1.1474688 5.143679e-01\n5  lsm_ref_Week 2 -3.29966757 0.6139776 -4.511086 -2.0882493 2.330387e-07\n6  lsm_alt_Week 2 -3.86772670 0.6154448 -5.082087 -2.6533665 2.383863e-09\n7      trt_Week 4 -1.23794505 0.9341607 -3.081702  0.6058116 1.868456e-01\n8  lsm_ref_Week 4 -4.56691815 0.6705840 -5.890893 -3.2429436 1.703692e-10\n9  lsm_alt_Week 4 -5.80486320 0.6575761 -7.102678 -4.5070482 1.090291e-15\n10     trt_Week 6 -1.69994606 1.0170780 -3.707618  0.3077255 9.647476e-02\n11 lsm_ref_Week 6 -5.21952895 0.7327090 -6.666472 -3.7725863 3.306987e-11\n12 lsm_alt_Week 6 -6.91947501 0.7414260 -8.383999 -5.4549508 9.864655e-17\n13     trt_Week 8 -1.72945373 1.1031354 -3.908286  0.4493783 1.189418e-01\n14 lsm_ref_Week 8 -5.31618579 0.8094870 -6.916486 -3.7158856 9.139985e-10\n15 lsm_alt_Week 8 -7.04563951 0.8266465 -8.680733 -5.4105457 2.995529e-14\n\n\n\n\n\n\nCarpenter, Roger, J. R., and M. G. Kenward. 2013. “Analysis of Longitudinal Trials with Protocol Deviation: A Framework for Relevant, Accessible Assumptions, and Inference via Multiple Imputation.” Journal of Biopharmaceutical Statistics, October. https://doi.org/10.1080/10543406.2013.834911.\n\n\nGower-Page, Noci, C., and M. Wolbers. 2022. “Rbmi: A r Package for Standard and Reference-Based Multiple Imputation Methods.” Journal of Open Source Software, June. https://doi.org/10.21105/joss.04251.\n\n\nKayembe, Jolani, M. T., and G. J. P. van Breukelen. 2022. “Imputation of Missing Covariates in Randomized Controlled Trials with Continuous Outcomes: Simple, Unbiased and Efficient Methods.” Journal of Biopharmaceutical Statistics, September. https://doi.org/10.1080/10543406.2021.2011898."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Carpenter, Roger, J. R., and M. G. Kenward. 2013. “Analysis of\nLongitudinal Trials with Protocol Deviation: A Framework for Relevant,\nAccessible Assumptions, and Inference via Multiple Imputation.”\nJournal of Biopharmaceutical Statistics, October. https://doi.org/10.1080/10543406.2013.834911.\n\n\nCraig Mallinckrodt, Ilya Lipkovich. 2016. Analyzing Longitudinal\nClinical Trial Data: A Practical Guide. Vol. 1. USA: Chapman;\nHall/CRC. https://doi.org/10.1201/9781315186634.\n\n\nFitzmaurice, Laird, G. M. 2011. Applied Longitudinal Analysis.\nVol. 2. USA: New York, Wiley. https://doi.org/10.1002/9781119513469.\n\n\nGower-Page, Noci, C., and M. Wolbers. 2022. “Rbmi: A r Package for\nStandard and Reference-Based Multiple Imputation Methods.”\nJournal of Open Source Software, June. https://doi.org/10.21105/joss.04251.\n\n\nKayembe, Jolani, M. T., and G. J. P. van Breukelen. 2022.\n“Imputation of Missing Covariates in Randomized Controlled Trials\nwith Continuous Outcomes: Simple, Unbiased and Efficient\nMethods.” Journal of Biopharmaceutical Statistics,\nSeptember. https://doi.org/10.1080/10543406.2021.2011898.\n\n\nMallinckrodt, Craig. 2016. Analyzing Longitudinal Clinical Trial\nData. Chapman; Hall/CRC. https://doi.org/10.1201/9781315186634.\n\n\nRoderick JA Little, Donald B. Rubin. 2019. Statistical Analysis with\nMissing Data. Vol. 3. USA: New York, Wiley. https://doi.org/10.1002/9781119482260."
  }
]