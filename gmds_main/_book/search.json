[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Longitudinal Data Modeling",
    "section": "",
    "text": "Preface"
  },
  {
    "objectID": "intro.html#workshop-structure",
    "href": "intro.html#workshop-structure",
    "title": "1  Introduction",
    "section": "1.1 Workshop Structure",
    "text": "1.1 Workshop Structure\nThis class focuses on the longitudinal modeling of data from Patient Reported Outcomes (PROs). It is meant to be hands-on class with applications in R.\nContent and structure follow the book by (Mallinckrodt 2016). We would like to extend our warmest gratitude towards Dr. Mallinckrodt for providing the example data for the workshop.\nThe following topics will be covered:\n\nWelcome and Introduction (WS session 1)\nExploration and visualization of longitudinal data (WS session 1/2)\nInferences from longitudinal data (WS session 3 + 4)\nAssessment of missingness patterns (WS session 5)\nSensitivity analyses to assess the impact of missingness (WS session 6)\nAnnex: Inferences from longitudinal binary data (WS session 7)"
  },
  {
    "objectID": "intro.html#longitudinal-data",
    "href": "intro.html#longitudinal-data",
    "title": "1  Introduction",
    "section": "1.2 Longitudinal Data",
    "text": "1.2 Longitudinal Data\nThis workshop focuses on the analysis of data observed in randomized clinical trials (RCTs). Here, patients have assessments taken at the start of their treatment and then subsequently throughout the course of the trial based on a pre-specified schedule of assessments. The measurement at the start of the treatment is usually referred to as the baseline.\nResearchers can be interested in\n\nthe occurrence of a certain event during the course of the trial, e.g. death or a cardiac event, or the time to the occurrence of such an event, or\nthe longitudinal profile from multiple repeated measurements taken, with a focus on either estimates at a landmark visit or across several time points.\n\nThe outcomes under point 1. can be handled via a comparison of the percentages of patients with events between treatment arms, or a time-to-event analysis. Both are out of scope of this workshop."
  },
  {
    "objectID": "intro.html#basics-about-rstudio-pre-read",
    "href": "intro.html#basics-about-rstudio-pre-read",
    "title": "1  Introduction",
    "section": "1.3 Basics about RStudio (pre-read)",
    "text": "1.3 Basics about RStudio (pre-read)\nAlex to add pre-read (YouTube + Cheat Sheets)"
  },
  {
    "objectID": "intro.html#example-data",
    "href": "intro.html#example-data",
    "title": "1  Introduction",
    "section": "1.4 Example data",
    "text": "1.4 Example data\n\n\n\n\nMallinckrodt, Craig. 2016. Analyzing Longitudinal Clinical Trial Data. Chapman; Hall/CRC. https://doi.org/10.1201/9781315186634."
  },
  {
    "objectID": "s1_visualization.html",
    "href": "s1_visualization.html",
    "title": "2  Longitudinal Data Exploration and Visualization",
    "section": "",
    "text": "3 Correlation structure, covariance matrices"
  },
  {
    "objectID": "s1_visualization.html#introduction",
    "href": "s1_visualization.html#introduction",
    "title": "2  Longitudinal Data Exploration and Visualization",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\n\nData on individuals followed over time with information collected at several time points.\nClusters are the individuals who are followed over time.\nRepeated observations may or may not be taken at regular times (balanced, fixed occasions, do not differ between subjects).\nOur interest is in the change from baseline.\n\nDatasets used in this course: - Example data is taken from (Mallinckrodt 2016). The authors generated data sets based on two nearly identically designed antidepressant clinical trials by randomly selecting subjects from the original data. - Contain data on the continuous variable HAMD17 (Hamilton 17-item rating scale for depression). - Two treatement arms are included: placebo (arm 1) vs. drug (arm 2). - Assessments were taken at baseline and weeks 1, 2, 4, 6, and 8.\nThere are 3 data sets created from the original data: - Data all2 = Subsample of the large dataset with n=50, visits: weeks 2, 4, 8 - Data high2 = Large dataset with n=100, high dropout = 70% (drug), 60% (placebo) - Data low2 = Large dataset with n=100, low dropout = 18%"
  },
  {
    "objectID": "s1_visualization.html#data-set-all2",
    "href": "s1_visualization.html#data-set-all2",
    "title": "2  Longitudinal Data Exploration and Visualization",
    "section": "2.2 Data set all2",
    "text": "2.2 Data set all2\n\nSmall data set with n=50 subjects.\n1st version: complete data where all subjects adhered to the originally assigend study medication, variable change\n2nd version = missing data: identical to the first except some data were missing (drop-out), variable chgdrop\n\nLooking at the variables in the data set\n\nhead(all2)\n\n# A tibble: 6 × 14\n  subject  time chgdrop trt   basval change pgiimp gender chgrescue dropout_grp \n  &lt;fct&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;       \n1 1           1     -11 2         24    -11      3 F            -11 Week 2 Drop…\n2 1           2      NA 2         24    -16      2 F            -26 Week 2 Drop…\n3 1           3      NA 2         24    -24      2 F            -34 Week 2 Drop…\n4 2           1      -6 1         20     -6      4 F             -6 Week 2 Drop…\n5 2           2      NA 1         20     -8      4 F            -18 Week 2 Drop…\n6 2           3      NA 1         20     -5      5 F            -15 Week 2 Drop…\n# ℹ 4 more variables: aval &lt;dbl&gt;, avisit &lt;fct&gt;, week &lt;dbl&gt;, group &lt;fct&gt;\n\n\n\n2.2.1 Task 1 - Exploration of data set all2 - 15 minutes working time\nOnly consider the complete data, variable change - Are the data balanced and equally spaced? - Number of observations by week? - Summary statistics for HAMD17 (change from baseline) by week. - Plot trajectories for each individual, different colors for each treatment group (or panels). Add mean to your plot. ODER Mean plot separat evtl. mit CI - Generate and interpret the group-wise boxplots of the change from baseline. - Plot trajectories separately by gender. Comment on the plots.\n\n\n2.2.2 Task 1 Discussion, possible solution\nTable: Summary statistics for HAMD17 by treatment and week in the all2 data set\n\nall2 %&gt;%\n  select(change, group, avisit) %&gt;%\n  tbl_strata(strata=group, \n             ~.x %&gt;% \n               tbl_summary(by = avisit,\n                           statistic = list(\n      all_continuous() ~ \"{mean} ({sd})\"), \n      digits = all_continuous() ~ 2 ) \n)\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      \n        Arm 1\n      \n      \n        Arm 2\n      \n    \n    \n      Week 2, N = 251\n      Week 4, N = 251\n      Week 8, N = 251\n      Week 2, N = 251\n      Week 4, N = 251\n      Week 8, N = 251\n    \n  \n  \n    change\n-4.20 (3.66)\n-6.80 (4.25)\n-9.88 (4.85)\n-5.24 (5.49)\n-8.60 (5.39)\n-13.24 (5.54)\n  \n  \n  \n    \n      1 Mean (SD)\n    \n  \n\n\n\n\nFigure: trajectories\n\nggplot(data = all2, aes(x = week, y = change, group=subject)) +\n  geom_point() + geom_line() + facet_grid(.~group) + ylab(\"Change from baseline HAMD17\") +\n  scale_x_continuous(name=\"Visit [week]\", breaks=c(2,4,8))\n\n\n\n\nFigure 2.1: Individual trajectories of HAMD17 by treatment group\n\n\n\n\n\nggplot(data = all2, aes(x = week, y = change)) +  \n  geom_point(aes(colour=factor(group))) + ylab(\"Change from baseline HAMD17\") +\n  scale_x_continuous(name=\"Visit [week]\", breaks=c(2,4,8)) +\n  stat_summary(aes(group = group, colour=factor(group)), geom = \"line\", fun.y = mean,\n               size = 1) +\n  stat_summary(aes(group = group, colour=factor(group)), geom = \"point\", fun.y = mean,\n               shape=17,size = 2)\n\nWarning: The `fun.y` argument of `stat_summary()` is deprecated as of ggplot2 3.3.0.\nℹ Please use the `fun` argument instead.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nFigure 2.2: Mean HAMD17 change from baseline by treatment group"
  },
  {
    "objectID": "s1_visualization.html#data-set-high2",
    "href": "s1_visualization.html#data-set-high2",
    "title": "2  Longitudinal Data Exploration and Visualization",
    "section": "2.3 Data set high2",
    "text": "2.3 Data set high2\n\nLarge data set with n=100 subjects.\nNote that we have no intermittent missing values but drop-outs.\n\nLooking at the variables in the data set.\n\nhead(high2)\n\n# A tibble: 6 × 14\n# Groups:   patient [2]\n  patient trt   poolinv basval  week change pgiimp   age gender  aval  drop\n    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1    1401 1     005         19     1     -7      3  44.5 F         12     2\n2    1401 1     005         19     2     -4      3  44.5 F         15     2\n3    1411 2     005         17     1      0      3  35.7 F         17     8\n4    1411 2     005         17     2     -2      3  35.7 F         15     8\n5    1411 2     005         17     4      2      3  35.7 F         19     8\n6    1411 2     005         17     6     -3      2  35.7 F         14     8\n# ℹ 3 more variables: group &lt;fct&gt;, avisit &lt;fct&gt;, dropgr &lt;fct&gt;\n\n\n\n2.3.1 Task 2 - Exploration of data set high2 - 15 minutes working time\n\ndrop-outs, last visit for each subject\ntrajectories for different drop-out groups\n\n\n\n2.3.2 Task 2 Discussion, possible solution\n\nggplot(data = high2, aes(x = week, y = change, group=patient)) + \n  geom_point() + geom_line() + facet_grid(.~group) +\n  ylab(\"Change from baseline HAMD17\") + scale_x_continuous(name=\"Visit [week]\", breaks=c(1,2,4,6,8))\n\n\n\n\nFigure 2.3: Individual trajectories of HAMD17 by treatment group\n\n\n\n\n\nggplot(data = high2, aes(x = week, y = change, group=patient)) + \n  geom_point(col=\"lightgray\") + geom_line(col=\"lightgray\") + facet_grid(.~group) +\n  ylab(\"Change from baseline HAMD17\") + scale_x_continuous(name=\"Visit [week]\", breaks=c(1,2,4,6,8)) +\n  stat_summary(aes(group = dropgr, colour=factor(dropgr)), geom = \"line\", fun.y = mean,\n               size = 1) +\n  stat_summary(aes(group = dropgr, colour=factor(dropgr)), geom = \"point\", fun.y = mean,\n               shape=17,size = 2)\n\n\n\n\nFigure 2.4: Visit-wise mean HAMD17 changes from baseline by treatment group and drop-out"
  },
  {
    "objectID": "s1_visualization.html#overview---different-covariance-matrices",
    "href": "s1_visualization.html#overview---different-covariance-matrices",
    "title": "2  Longitudinal Data Exploration and Visualization",
    "section": "3.1 Overview - different covariance matrices",
    "text": "3.1 Overview - different covariance matrices\n\nVariance components (VC) independence structure\nCompound symmetry (CS) also known as exchangeable\nToeplitz (TOEP)\nFirst order auto regressive (AR(1))\nUnstructured (UN)\n\nSelected covariance structures for data with three assessment times (t=3) are shown below. Note that with three assessment times, the number of parameters estimated for the various structures did not differ as much as would be the case with more assessment times. Thus, results from different covariance structures are more similar than would be the case with more assessment times.\n\n3.1.1 Independence structure (VC)\nConstant variance. It is assumed to be no correlation between assessments (residuals are independent across time). \\[ \\begin{bmatrix}\n   \\sigma^2   & 0  & 0  \\\\\n   0  & \\sigma^2   & 0 \\\\\n   0  & 0  & \\sigma^2\n   \\end{bmatrix}\\]\n\n\n3.1.2 Compound symmetry (CS)\nConstant variance and constant covariance across all assessments. Also known as exchangeable. It requires two parameter estimates. Most simplest repeated measures (i.e., correlated errors) structure.\n\\[ R = \\begin{bmatrix}\n   \\sigma^2 + \\sigma_1 & \\sigma_1  & \\sigma_1  \\\\\n   \\sigma_1  & \\sigma^2 + \\sigma_1  & \\sigma_1  \\\\\n   \\sigma_1  & \\sigma_1  & \\sigma^2 + \\sigma_1\n   \\end{bmatrix}\\]\n\n\n3.1.3 Unstructured (UN)\nThis is the most general (saturated) model. It has t + [t(t-1)/2] parameters to be estimated. Here it is 3 + 3 = 6 parameters.\n\\[ R = \\begin{bmatrix}\n   \\sigma_1^2 & \\sigma_{21}  & \\sigma_{31}  \\\\\n   \\sigma_{21}  & \\sigma_2^2   & \\sigma_{32}  \\\\\n   \\sigma_{31}  & \\sigma_{32}  & \\sigma_3^2\n   \\end{bmatrix}\\]\n\n\n3.1.4 Toeplitz structure (TOEP)\nHomogenous variances and heterogenous correlations. Same correlation value is used whenever the degree of adjacency is the same e.g. correlation between times 1 and 2 = correlation between times 2 and 3. Repeated measurements are assumed to be equally spaced. TOEP requries t parameter estimates so here we have t=3 parameter.\n\\[ R = \\begin{bmatrix}\n   \\sigma^2  & \\sigma_1^2 & \\sigma_2^2 \\  \\\\\n   \\sigma_1^2 & \\sigma^2  & \\sigma_1^2  \\\\\n   \\sigma_2^2  & \\sigma_1^2 & \\sigma^2\n   \\end{bmatrix}\\]\n\n\n3.1.5 Autoregressive structure (AR(1))\nCorrelation decreases as time between observations increases. Assumtpion of equal spacing between each repeated measurement must be reasonably applicable. This structure requires the estimation of two parameters.\n\\[ R = \\begin{bmatrix}\n   \\sigma^2  & \\sigma^2 \\rho  & \\sigma^2 \\rho^2  \\\\\n   \\sigma^2 \\rho & \\sigma^2   & \\sigma^2 \\rho  \\\\\n  \\sigma^2 \\rho^2  & \\sigma^2 \\rho  & \\sigma^2\n   \\end{bmatrix}\\]\n\n\n3.1.6 Spatial Power (SP)\nSpatial covariance structures does not require equal spacing between measurements. Instead, as long as the distance between visits can be quantified in terms of time and/or other coordinates, the spatial covariance structure can be applied. Covariances are mathematical functions of Euclidean distances between observed measurements. Again, two parameters need to be estimated.\nFor spatial exponential, the covariance structure is defined as follows:\n\\[ R = \\begin{bmatrix}\n   \\sigma^2  & \\sigma^2 \\rho_{12}  & \\sigma^2 \\rho_{13}  \\\\\n   \\sigma^2 \\rho_{21} & \\sigma^2   & \\sigma^2 \\rho_{23}  \\\\\n  \\sigma^2 \\rho_{31}  & \\sigma^2 \\rho_{32}  & \\sigma^2\n   \\end{bmatrix}\\]\nwith \\[ \\rho_{ij}=\\rho^{d_{ij}} \\] where \\[d_{ij} \\] is the distance between time point i and time point j e.g. distance in weeks."
  },
  {
    "objectID": "s1_visualization.html#selecting-the-covariance-structure",
    "href": "s1_visualization.html#selecting-the-covariance-structure",
    "title": "2  Longitudinal Data Exploration and Visualization",
    "section": "3.2 Selecting the covariance structure",
    "text": "3.2 Selecting the covariance structure\nThere are a variety of considerations when selecting the covariance structure: - number of parameters - interpretation of the structure - model fit\nUN is the most flexible (complex) structure and can fail to run especially if one has many repeated measures. Choose a reasonable covaraiance structure which is the best compromise between model fit and complexity. E.g. use AIC as it penalises more complex models."
  },
  {
    "objectID": "s1_visualization.html#task-3---exploration-of-correlation-in-the-data",
    "href": "s1_visualization.html#task-3---exploration-of-correlation-in-the-data",
    "title": "2  Longitudinal Data Exploration and Visualization",
    "section": "3.3 Task 3 - Exploration of correlation in the data",
    "text": "3.3 Task 3 - Exploration of correlation in the data\n\nCompute the empirical correlations between measurement timepoints in the all2 data set (e.g. correlation between baseline and post-baseline changes).\nLooking at these correlations + using your knowledge of the experiment (e.g., spacing of measurements), comment on the suitability of the correlation structures VC, CS, UN, AR(1)."
  },
  {
    "objectID": "s1_visualization.html#task-3---discussion-and-possible-solution",
    "href": "s1_visualization.html#task-3---discussion-and-possible-solution",
    "title": "2  Longitudinal Data Exploration and Visualization",
    "section": "3.4 Task 3 - Discussion and possible solution",
    "text": "3.4 Task 3 - Discussion and possible solution\nTable: Correlation and covariance matrix\n\nall2 %&gt;% pivot_wider(id_cols=subject,names_from = time, values_from = c(basval,change)) %&gt;% \n  select(-c(basval_2,basval_3)) -&gt; all2.w\n\ncor(all2.w[-1]) \n\n            basval_1   change_1   change_2    change_3\nbasval_1  1.00000000 -0.2636447 -0.3165711 -0.02915138\nchange_1 -0.26364471  1.0000000  0.7557078  0.51502724\nchange_2 -0.31657106  0.7557078  1.0000000  0.71298768\nchange_3 -0.02915138  0.5150272  0.7129877  1.00000000\n\ncov(all2.w[-1])\n\n           basval_1  change_1  change_2   change_3\nbasval_1 16.3330612 -4.955918 -6.253061 -0.6391837\nchange_1 -4.9559184 21.634286 17.179592 12.9967347\nchange_2 -6.2530612 17.179592 23.887755 18.9061224\nchange_3 -0.6391837 12.996735 18.906122 29.4351020\n\n\n\n\n\n\nMallinckrodt, Craig. 2016. Analyzing Longitudinal Clinical Trial Data. Chapman; Hall/CRC. https://doi.org/10.1201/9781315186634."
  },
  {
    "objectID": "s2_inference_continuous.html#categorical-time",
    "href": "s2_inference_continuous.html#categorical-time",
    "title": "3  Inference from Longitudinal Data",
    "section": "3.1 Categorical Time",
    "text": "3.1 Categorical Time\nYou can start and familiarise yourself with the main function mmrm() using the command\n\nlibrary(mmrm)\n?mmrm\n\nTwo inputs are strictly required to get mmrm() to work:\n\nA model formula\nThe dataset, containing the response, as well as all fixed effects and variables in the covariance matrix.\n\nExercise: Fit a model fit_cat_time using the dataset all2, with change as dependent variable, baseline value, visit, baseline by visit interaction and treatment by visit interaction as fixed effects and an unstructured covariance matrix for visits within each subject.\n\nHow do different choices for covariance matrices change the results? What is the difference on the estimation procedure?\nYou can obtain a summary of the fit results via summary(fit_cat_time). How do you interpret the fit summary?\nLook at the structure of the fit summary and try to extract the estimate of the \\(R\\) matrix.\n\n\nfit_cat_time &lt;- mmrm::mmrm(\n  formula = change ~ basval*avisit + trt*avisit + us(avisit | subject),\n  data = all2,\n  control = mmrm_control(method = \"Kenward-Roger\")\n)\n\nsummary(fit_cat_time)\n\nmmrm fit\n\nFormula:     change ~ basval * avisit + trt * avisit + us(avisit | subject)\nData:        all2 (used 150 observations from 50 subjects with maximum 3 \ntimepoints)\nCovariance:  unstructured (6 variance parameters)\nMethod:      Kenward-Roger\nVcov Method: Kenward-Roger\nInference:   REML\n\nModel selection criteria:\n     AIC      BIC   logLik deviance \n   822.4    833.9   -405.2    810.4 \n\nCoefficients: \n                     Estimate Std. Error        df t value Pr(&gt;|t|)   \n(Intercept)           1.98452    3.27479  47.00000   0.606  0.54743   \nbasval               -0.31235    0.15905  47.00000  -1.964  0.05548 . \navisitWeek 4         -0.90862    2.39866  47.00000  -0.379  0.70654   \navisitWeek 8        -10.58630    3.45922  47.00000  -3.060  0.00365 **\ntrt2                 -1.18993    1.27265  47.00000  -0.935  0.35457   \nbasval:avisitWeek 4  -0.08542    0.11650  47.00000  -0.733  0.46704   \nbasval:avisitWeek 8   0.24779    0.16801  47.00000   1.475  0.14691   \navisitWeek 4:trt2    -0.80100    0.93217  47.00000  -0.859  0.39454   \navisitWeek 8:trt2    -2.20106    1.34432  47.00000  -1.637  0.10825   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCovariance estimate:\n        Week 2  Week 4  Week 8\nWeek 2 20.6112 15.3034 12.2766\nWeek 4 15.3034 21.3565 17.6648\nWeek 8 12.2766 17.6648 27.6127\n\n\nWe can assess the structure of the fit summary via\n\nstr(summary(fit_cat_time))\n\nList of 15\n $ cov_type        : chr \"us\"\n $ reml            : logi TRUE\n $ n_groups        : int 1\n $ n_theta         : int 6\n $ n_subjects      : int 50\n $ n_timepoints    : int 3\n $ n_obs           : int 150\n $ beta_vcov       : num [1:9, 1:9] 10.724 -0.501 -2.675 -4.267 -1.047 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:9] \"(Intercept)\" \"basval\" \"avisitWeek 4\" \"avisitWeek 8\" ...\n  .. ..$ : chr [1:9] \"(Intercept)\" \"basval\" \"avisitWeek 4\" \"avisitWeek 8\" ...\n $ varcor          : num [1:3, 1:3] 20.6 15.3 12.3 15.3 21.4 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:3] \"Week 2\" \"Week 4\" \"Week 8\"\n  .. ..$ : chr [1:3] \"Week 2\" \"Week 4\" \"Week 8\"\n $ method          : chr \"Kenward-Roger\"\n $ vcov            : chr \"Kenward-Roger\"\n $ coefficients    : num [1:9, 1:5] 1.985 -0.312 -0.909 -10.586 -1.19 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:9] \"(Intercept)\" \"basval\" \"avisitWeek 4\" \"avisitWeek 8\" ...\n  .. ..$ : chr [1:5] \"Estimate\" \"Std. Error\" \"df\" \"t value\" ...\n $ n_singular_coefs: int 0\n $ aic_list        :List of 4\n  ..$ AIC     : num 822\n  ..$ BIC     : num 834\n  ..$ logLik  : num -405\n  ..$ deviance: num 810\n $ call            : language mmrm::mmrm(formula = change ~ basval * avisit + trt * avisit + us(avisit |      subject), data = all2, control = | __truncated__\n - attr(*, \"class\")= chr \"summary.mmrm\"\n\n\nand then extract the covariance matrix\n\nsummary(fit_cat_time)$varcor\n\n         Week 2   Week 4   Week 8\nWeek 2 20.61117 15.30339 12.27661\nWeek 4 15.30339 21.35648 17.66478\nWeek 8 12.27661 17.66478 27.61271"
  },
  {
    "objectID": "s2_inference_continuous.html#continuous-time",
    "href": "s2_inference_continuous.html#continuous-time",
    "title": "3  Inference from Longitudinal Data",
    "section": "3.2 Continuous Time",
    "text": "3.2 Continuous Time\nTime as continuous effect -&gt; single df for time and trt-by-time interaction\nModeling: - Need avisit for structure of covariance matrix - Implicit assumption is for the covariance between values for two timepoints to be equal, regardless of the specific timing\n\nfit_cont_time &lt;- mmrm::mmrm(\n  formula = change ~ basval*time + trt*time + us(avisit | subject),\n  weights = all2$time,\n  data = all2,\n  control = mmrm_control(method = \"Kenward-Roger\")\n)\n\nQuadratic trend\n\nall2$timesq &lt;- all2$time^2\n\nfit_cont_timesq &lt;- mmrm::mmrm(\n  formula = change ~ basval*timesq + trt*timesq + us(avisit | subject),\n  weights = all2$time,\n  data = all2,\n  control = mmrm_control(method = \"Kenward-Roger\")\n)\n\nmodel checks - residuals per time point"
  },
  {
    "objectID": "s2_inference_continuous.html#baseline-as-a-response-clda-lda",
    "href": "s2_inference_continuous.html#baseline-as-a-response-clda-lda",
    "title": "3  Inference from Longitudinal Data",
    "section": "3.3 Baseline as a Response (cLDA + LDA)",
    "text": "3.3 Baseline as a Response (cLDA + LDA)"
  },
  {
    "objectID": "s2_inference_continuous.html#adjusted-ls-means-from-mmrms",
    "href": "s2_inference_continuous.html#adjusted-ls-means-from-mmrms",
    "title": "3  Inference from Longitudinal Data",
    "section": "3.4 (Adjusted) LS Means from MMRMs",
    "text": "3.4 (Adjusted) LS Means from MMRMs\nLS Means are means of the dependent variable adjusted for covariates in the statistical model. We can obtain LS Means estimates and contrasts allowing for a treatment comparison using the emmeans package.\nExample: Calculate the observed (raw) means of changes along with number of patients by treatment group from the dataset all2 overall and by visit. Then take the model fit_cat_time and derive the respective LS Means from the model. What do you observe?\n\n# Raw means\n\nall2 %&gt;% \n  dplyr::group_by(group) %&gt;% \n  dplyr::summarise(\n    N = dplyr::n(),\n    Mean = mean(change),\n    .groups = \"drop\"\n  )\n\n# A tibble: 2 × 3\n  group     N  Mean\n  &lt;fct&gt; &lt;int&gt; &lt;dbl&gt;\n1 Arm 1    75 -6.96\n2 Arm 2    75 -9.03\n\nall2 %&gt;% \n  dplyr::group_by(group, avisit) %&gt;% \n  dplyr::summarise(\n    N = dplyr::n(),\n    Mean = mean(change),\n    .groups = \"drop\"\n  )\n\n# A tibble: 6 × 4\n  group avisit     N   Mean\n  &lt;fct&gt; &lt;fct&gt;  &lt;int&gt;  &lt;dbl&gt;\n1 Arm 1 Week 2    25  -4.2 \n2 Arm 1 Week 4    25  -6.8 \n3 Arm 1 Week 8    25  -9.88\n4 Arm 2 Week 2    25  -5.24\n5 Arm 2 Week 4    25  -8.6 \n6 Arm 2 Week 8    25 -13.2 \n\n\nThe respective LS Means from the model with time as a fixed factor yields the following estimates:\n\nlibrary(emmeans)\n\nemmeans::ref_grid(fit_cat_time)\n\n'emmGrid' object with variables:\n    basval = 19.56\n    avisit = Week 2, Week 4, Week 8\n    trt = 1, 2\n\nemmeans(fit_cat_time, ~trt)\n\nNOTE: Results may be misleading due to involvement in interactions\n\n\n trt emmean    SE df lower.CL upper.CL\n 1    -6.90 0.836 47    -8.58    -5.22\n 2    -9.09 0.836 47   -10.77    -7.41\n\nResults are averaged over the levels of: avisit \nConfidence level used: 0.95 \n\nemmeans(fit_cat_time, ~trt*avisit)\n\n trt avisit emmean    SE df lower.CL upper.CL\n 1   Week 2  -4.13 0.899 47    -5.93    -2.32\n 2   Week 2  -5.31 0.899 47    -7.12    -3.51\n 1   Week 4  -6.70 0.916 47    -8.55    -4.86\n 2   Week 4  -8.70 0.916 47   -10.54    -6.85\n 1   Week 8  -9.86 1.033 47   -11.94    -7.79\n 2   Week 8 -13.26 1.033 47   -15.33   -11.18\n\nConfidence level used: 0.95 \n\n\n\n3.4.1 Observed vs. balanced margins\nIn the example above we have used the standard option for the weights in the calculation of LS Means. We will delve deeper into the following two options and will try to understand the difference:\n\nweights = \"equal\": Each stratum induced by covariate levels is assigned the same weight in the calculation of the LS Means. This is the default option.\nweights = \"proportional\": Each stratum induced by covariate levels is assigned a weight according to their observed proportion in the calculation of the LS Mean. This option gives each stratum a weight corresponding to its size. Estimates using this option are reflective of the balance of covariates in the data.\n\nExercise: Based on the fit_cat_time model, compare the LS Means for the change in the response variable by treatment overall and treatment by visit interaction using the different options for weight. Compare the results for the two LS Means options to the observed means and to one another.\nDiscuss the following points:\n\nWhy is there no difference between LS Means estimates for the overall treatment effect and the treatment by visit interaction? (Hint: Create a frequency table)\n\nNow update the fit_cat_time model to fit_cat_time2, and include the covariate gender. Estimate the same LS Means for the change in the response variable by treatment (overall) and treatment by visit interaction.\n\nWhy is there a difference now between results from the different LS Means options? (Hint: another frequency table can help)\nWhat effect could missing data have on the estimation, even in the case of fit_cat_time? I.e. what would happen if this data was not complete but subject to missingness, with the degree of missing data increasing over time and being disproportionate between treatment arms?\n\nSolution:\nWe first calculate the LS Means, using the different weights options and find they are indeed identical.\n\n# These will yield the same results:\nemmeans(fit_cat_time, ~trt, weights = \"equal\")\n\nNOTE: Results may be misleading due to involvement in interactions\n\n\n trt emmean    SE df lower.CL upper.CL\n 1    -6.90 0.836 47    -8.58    -5.22\n 2    -9.09 0.836 47   -10.77    -7.41\n\nResults are averaged over the levels of: avisit \nConfidence level used: 0.95 \n\nemmeans(fit_cat_time, ~trt, weights = \"proportional\")\n\nNOTE: Results may be misleading due to involvement in interactions\n\n\n trt emmean    SE df lower.CL upper.CL\n 1    -6.90 0.836 47    -8.58    -5.22\n 2    -9.09 0.836 47   -10.77    -7.41\n\nResults are averaged over the levels of: avisit \nConfidence level used: 0.95 \n\nemmeans(fit_cat_time, ~trt*avisit, weights = \"equal\")\n\n trt avisit emmean    SE df lower.CL upper.CL\n 1   Week 2  -4.13 0.899 47    -5.93    -2.32\n 2   Week 2  -5.31 0.899 47    -7.12    -3.51\n 1   Week 4  -6.70 0.916 47    -8.55    -4.86\n 2   Week 4  -8.70 0.916 47   -10.54    -6.85\n 1   Week 8  -9.86 1.033 47   -11.94    -7.79\n 2   Week 8 -13.26 1.033 47   -15.33   -11.18\n\nConfidence level used: 0.95 \n\nemmeans(fit_cat_time, ~trt*avisit, weights = \"proportional\")\n\n trt avisit emmean    SE df lower.CL upper.CL\n 1   Week 2  -4.13 0.899 47    -5.93    -2.32\n 2   Week 2  -5.31 0.899 47    -7.12    -3.51\n 1   Week 4  -6.70 0.916 47    -8.55    -4.86\n 2   Week 4  -8.70 0.916 47   -10.54    -6.85\n 1   Week 8  -9.86 1.033 47   -11.94    -7.79\n 2   Week 8 -13.26 1.033 47   -15.33   -11.18\n\nConfidence level used: 0.95 \n\n\nNow we can update the model to include the covariate gender. We can specify this a new model using the mmrm() function again, or simply use update() to add the new covariate to the model. Either way is fine, and a look into the model formula from the fit summary shows the two approaches work interchangeably.\n\nfit_cat_time2 &lt;- update(fit_cat_time, . ~ . + gender)\nsummary(fit_cat_time2)\n\nmmrm fit\n\nFormula:     \nchange ~ basval + avisit + trt + (us(avisit | subject)) + gender +  \n    basval:avisit + avisit:trt\nData:        all2 (used 150 observations from 50 subjects with maximum 3 \ntimepoints)\nCovariance:  unstructured (6 variance parameters)\nMethod:      Kenward-Roger\nVcov Method: Kenward-Roger\nInference:   REML\n\nModel selection criteria:\n     AIC      BIC   logLik deviance \n   817.0    828.5   -402.5    805.0 \n\nCoefficients: \n                     Estimate Std. Error        df t value Pr(&gt;|t|)   \n(Intercept)           0.47589    3.23944  46.14000   0.147  0.88385   \nbasval               -0.30674    0.15200  45.44000  -2.018  0.04951 * \navisitWeek 4         -0.90862    2.39786  47.00000  -0.379  0.70645   \navisitWeek 8        -10.58630    3.45626  47.00000  -3.063  0.00362 **\ntrt2                 -0.34868    1.30287  46.74000  -0.268  0.79016   \ngenderM               2.32931    1.29556  45.99000   1.798  0.07876 . \nbasval:avisitWeek 4  -0.08542    0.11646  47.00000  -0.734  0.46689   \nbasval:avisitWeek 8   0.24779    0.16786  47.00000   1.476  0.14657   \navisitWeek 4:trt2    -0.80100    0.93186  47.00000  -0.860  0.39439   \navisitWeek 8:trt2    -2.20106    1.34318  47.00000  -1.639  0.10795   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCovariance estimate:\n        Week 2  Week 4  Week 8\nWeek 2 18.8295 14.3160 12.0002\nWeek 4 14.3160 21.1623 18.1813\nWeek 8 12.0002 18.1813 28.8384\n\n\nA look into the reference grid shows us the new factor levels for gender. Note that gender itself will not be included in the emmeans() statement, but the output indicates the averaging over its levels (same for the levels of avisit)\n\n# Reference grid shows us the new levels\nemmeans::ref_grid(fit_cat_time2)\n\n'emmGrid' object with variables:\n    basval = 19.56\n    avisit = Week 2, Week 4, Week 8\n    trt = 1, 2\n    gender = F, M\n\n# These two won't yield the same results\nemmeans(fit_cat_time2, ~trt, weights = \"equal\")\n\nNOTE: Results may be misleading due to involvement in interactions\n\n\n trt emmean    SE   df lower.CL upper.CL\n 1    -7.13 0.840 45.8    -8.82    -5.44\n 2    -8.48 0.896 46.4   -10.28    -6.68\n\nResults are averaged over the levels of: avisit, gender \nConfidence level used: 0.95 \n\nemmeans(fit_cat_time2, ~trt*avisit, weights = \"proportional\")\n\n trt avisit emmean    SE   df lower.CL upper.CL\n 1   Week 2  -4.55 0.890 46.2    -6.34    -2.75\n 2   Week 2  -4.89 0.890 46.2    -6.69    -3.10\n 1   Week 4  -7.13 0.941 47.1    -9.02    -5.23\n 2   Week 4  -8.27 0.941 47.1   -10.17    -6.38\n 1   Week 8 -10.29 1.081 46.6   -12.46    -8.11\n 2   Week 8 -12.83 1.081 46.6   -15.01   -10.66\n\nResults are averaged over the levels of: gender \nConfidence level used: 0.95 \n\n\nThe following frequency table shows the imbalance in the distribution of the gender variable. We can see that Treatment 1 has more men than women, whereas Treatment 2 has more women than men.\n\ntable(all2$trt, all2$gender)\n\n   \n     F  M\n  1 30 45\n  2 57 18\n\n\nThe data is no longer balanced across the covariates in the model. The weights = \"equal\" option is agnostic to this imbalance and assigns all levels equal weights, whereas the weights = \"proportional\" assigns a weight reflecting the proportional size of the stratum over which the average is taken.\n\n\n3.4.2 Contrasts"
  },
  {
    "objectID": "s2_inference_continuous.html#fit-diagnostics",
    "href": "s2_inference_continuous.html#fit-diagnostics",
    "title": "3  Inference from Longitudinal Data",
    "section": "3.5 Fit diagnostics",
    "text": "3.5 Fit diagnostics\nThe following section closely follows the content in Chapter 10 in (Fitzmaurice 2011).\nOur analysis should be concluded with a look into the fit diagnostics, more specifically, the residuals. Residuals are defined by the difference between the true responses and the fitted values from the model:\n\\[\nr := y - X\\hat\\beta\\,,\n\\] where \\(\\hat\\beta\\) are the estimated coefficients from our model. Residuals provide an estimate of the true vector of random errors\n\\[\n\\varepsilon = y - X\\beta\\,.\n\\]\nAs per our modeling assumptions, \\(\\varepsilon\\) should follow a normal distribution with mean zero. The mean of the residuals is zero and therefore identical with the mean of the error term. For the covariance of the residuals however, the variance-covariance matrix of \\(\\varepsilon\\) only serves us as an approximation (as suggested by (Fitzmaurice 2011) for all ‘practical applications’):\n\\[\nCov(r) \\approx Cov(\\varepsilon) = R\\,.\n\\] This assumption has several implications on the residual diagnostics:\n\nThe variance is not necessarily constant. Plotting the fitted values versus the residuals might therefore lead to a non-constant range. An examination of the residual variance or autocorrelation among residuals is therefore not very meaningful.\nResiduals from analyses of longitudinal data can exhibit correlation with the covariates. Scatterplots of residuals versus selected covariates can therefore reveal systematic trends (which normally should not be the case).\n\nA transformation of residuals to achieve constant variance and zero correlation is therefore often useful. This transformation uses the so-called Cholesky decomposition of the variance-covariance matrix \\(R\\). Let \\(L\\) be a lower triangular matrix, such that\n\\[\nR = L\\,L'\\,,\n\\] then the transformed residuals are given by \\[\nr^* =  L^{-1}(y - X\\beta)\\,.\n\\] In the mmrm package, transformed residuals can be derived using the type = \"normalized\" option.\nResidual plots overall and by visit\n\nhistogram of transformed residuals\npredicted vs. actual values\npredicted vs. residuals\nstandardized residuals vs. theoretic quantiles\n\nExercise: Which visualisations can you think of that make sense to assess the goodness of fit here? Create a new tibble (or data.frame) containing the variables of importance and try plotting them in a meaningful way. Discuss the results within your group.\nSolution:\nTo avoid repetition, let us first save the important variables to perform fit diagnostics in a tibble.\n\ndf_residuals &lt;- dplyr::tibble(\n  residuals = residuals(fit_cat_time, type = \"normalized\"),\n  predictions = fitted(fit_cat_time),\n  all2\n)\n\nWe can firstly look into a histogram of transformed residuals. The shape should resemble the density function of normal distribution with mean zero and positive variance. Superimposing the density function with mean and SD derived from the model residuals, let’s us see that this is indeed the case. We can also detect a slight skewness to the right.\n\nlibrary(ggplot2)\n\ndf_residuals %&gt;% \n  ggplot(aes(x = residuals)) +\n  geom_histogram(aes(y = after_stat(density)), fill='lightgray', col='black') +\n  stat_function(fun = dnorm, args = list(mean=mean(df_residuals$residuals), sd=sd(df_residuals$residuals)), col='red', lwd=1) +\n  ggtitle(\n    label = \"Histogram of transformed residuals\",\n    subtitle = \"Normal density with mean and SD of residuals superimposed\"\n  )\n\n\n\n\nAlternatively, we can create a Q-Q-Plot of …\n\ndf_residuals %&gt;% \n  ggplot(aes(x = predictions, y = residuals)) +\n  geom_point() +\n  geom_smooth(method = lm, color = \"blue\") +\n  geom_hline(yintercept = 0, show.legend = FALSE, linetype = 2) +\n  ggtitle(\n    label = \"Residual Plot of predicted values vs. transformed residuals\"\n  )\n\n\n\n\nWhat do we see?\n\nThe points in the plot look well dispersed and symmetric around zero. The fitted line shows no departure from zero.\nThere is no systematic trend, but a rather random scatter.\n\n\ndf_residuals %&gt;% \n  ggplot(aes(x = predictions, y = change)) +\n  geom_point() +\n  geom_smooth(method = lm, color = \"blue\")\n\n\n\n\n\n3.5.1 Addendum on RS&I Models\nDifferent dosing/ assessment frequency between treatment arms in parallel design -&gt; oncology (chemo with fixed cycles vs immune-therapy)\n\n\n\n\nFitzmaurice, Laird, G. M. 2011. Applied Longitudinal Analysis. Vol. 2. USA: New York, Wiley. https://doi.org/10.1002/9781119513469."
  },
  {
    "objectID": "s3_missingness.html#missing-data-mechanisms",
    "href": "s3_missingness.html#missing-data-mechanisms",
    "title": "4  Missing Data",
    "section": "4.1 Missing Data Mechanisms",
    "text": "4.1 Missing Data Mechanisms\nTo understand the nature of missing data in our clinical trial, we consider the following taxonomy, introduced by (Roderick JA Little 2019). We differentiate between the following three types of missing data:\n\nMissing Completely at Random (MCAR): Conditional on all covariates in our analysis, the probability of missingness does not depend on either observed or unobserved values of the response variable.\nMissing at Random (MAR): Conditional on all covariates and observed response values in our analysis, the probability of missingness does not depend on the unobserved values of the response variable.\nMissing not at Random (MNAR): Conditional on all covariates and observed response values in our analysis, the probability of missingness does depend on the unobserved values of the response variable.\n\n(Craig Mallinckrodt 2016) give the following interpretation around the three types of missingness:\n“With MCAR, the outcome variable is not related to the probability of dropout (after taking into account covariates). In MAR, the observed values of the outcome variable are related to the probability of dropout, but the unobserved outcomes are not (after taking into account covariates and observed outcomes). In MNAR the unobserved outcomes are related to the probability of dropout even after the observed outcomes and covariates have been taken into account.”\nThe following two sections outline handling strategies for missing data. However, the best approach to handle missing data is to minimise its extent. While the occurence of missing data can rarely be avoided at all (think about the collection of questionnaire data in oncology studies and the missing data after subjects die), it is important to pursue an “as complete as can be” data collection.\nBaseline and screening data are of utmost importance in a pursuit of data completeness. If a screening value is missing, but was meant to be used as a covariate, this subjects’ whole data will be dropped from the analysis even if all responses were observed. If the baseline response variable was missing we are unable to compute a change from baseline, which also leads to the loss of this subjects’ data in the model (although LDA models are still able to provide an estimate) even if all post-baseline values were observed."
  },
  {
    "objectID": "s3_missingness.html#missing-data-handling-i-descriptive-stats-visualisations",
    "href": "s3_missingness.html#missing-data-handling-i-descriptive-stats-visualisations",
    "title": "4  Missing Data",
    "section": "4.2 Missing data handling I (descriptive stats + visualisations)",
    "text": "4.2 Missing data handling I (descriptive stats + visualisations)"
  },
  {
    "objectID": "s3_missingness.html#missing-data-handling-ii-analytic-approaches",
    "href": "s3_missingness.html#missing-data-handling-ii-analytic-approaches",
    "title": "4  Missing Data",
    "section": "4.3 Missing data handling II (analytic approaches)",
    "text": "4.3 Missing data handling II (analytic approaches)\n\n4.3.1 Complete Case Analyses\n\n\n\n\nCraig Mallinckrodt, Ilya Lipkovich. 2016. Analyzing Longitudinal Clinical Trial Data: A Practical Guide. Vol. 1. USA: Chapman; Hall/CRC. https://doi.org/10.1201/9781315186634.\n\n\nRoderick JA Little, Donald B. Rubin. 2019. Statistical Analysis with Missing Data. Vol. 3. USA: New York, Wiley. https://doi.org/10.1002/9781119482260."
  },
  {
    "objectID": "s4_sensitivity_analyses.html#purpose-of-sensitivity-analyses",
    "href": "s4_sensitivity_analyses.html#purpose-of-sensitivity-analyses",
    "title": "5  Sensitivity Analyses",
    "section": "5.1 Purpose of sensitivity analyses",
    "text": "5.1 Purpose of sensitivity analyses\n\nConsider sensivitiy analyses to check model assumptions e.g. assumption of MAR.\nComparing results from sensitivity analyses: how much inference rely on the assumptions.\nHere, inference with regard to the treatment effect. Thus, investigate how treatment effects vary depending on assumptions (about missing data).\nUncertainty from incompleteness cannot be objectively evaluated from observed data so there is a need for missing data sensitivity analyses."
  },
  {
    "objectID": "s4_sensitivity_analyses.html#mmrm-vs.-mi",
    "href": "s4_sensitivity_analyses.html#mmrm-vs.-mi",
    "title": "5  Sensitivity Analyses",
    "section": "5.2 MMRM vs. MI",
    "text": "5.2 MMRM vs. MI\n\nFlexibility in modeling treatment effects over time and the within-patient error correlation structure makes MMRM a widely useful analysis.\nMMRM, MI: two major approaches to missing data with good statistical properties. Both rely on MAR assumption (for MI: standard implementation).\nMMRM: missing values implicitly imputed, MI: missing values explicitly imputed.\nMMRM vs. MI: approximately equivalent provided the variables used in the imputation model are the same as those included in the analysis model (level of equivalence will depend on the number of imputations)\nMI: imputation model with at least those variables from the primary model, additional auxiliary variables can be used in the imputation model to improve the accuracy of the missing data prediction.\nHandling missing not at random (MNAR) possible for MI (e.g. reference-based imputation) but not within MMRM.\nMMRM does not work if missing baseline values are present. Missing baseline values can be imputed first. Additionally, at least one post-baseline value has to be observed. Alternative: LDA where baseline is part of the response vector.\n\nNote that, when implemented in similar manners, MI and MMRM have similar assumptions and yield similar results. Thus, MI implemented similarly to MMRM is not a sensitivity analysis!"
  },
  {
    "objectID": "s4_sensitivity_analyses.html#missing-covariates-baseline-data",
    "href": "s4_sensitivity_analyses.html#missing-covariates-baseline-data",
    "title": "5  Sensitivity Analyses",
    "section": "5.3 Missing covariates (baseline data)",
    "text": "5.3 Missing covariates (baseline data)\n\nMissing baseline value of the outcome: MI or use of mean imputation (Paper: ),\nMMRM not efficient or potential biased estimates as subjects with missing covariates are excluded from the analysis"
  },
  {
    "objectID": "s4_sensitivity_analyses.html#sensitivity-analyses---simple-approaches",
    "href": "s4_sensitivity_analyses.html#sensitivity-analyses---simple-approaches",
    "title": "5  Sensitivity Analyses",
    "section": "5.4 Sensitivity analyses - Simple approaches",
    "text": "5.4 Sensitivity analyses - Simple approaches\nIn general, not recommended for use. Methods are of histroic interest and provide a useful starting point. - Last observation carried forward (LOCF): used in the past, justified as it was thought that it provides conservative estimates. - Complete case (observed case/completers analysis): creates selection bias, may cause overestimation of within group effects particularly at the last scheduled visit.\nWe apply these two methods here as well to compare results."
  },
  {
    "objectID": "s4_sensitivity_analyses.html#sensitivity-analyses---handling-nonignorable-missingness-mnar",
    "href": "s4_sensitivity_analyses.html#sensitivity-analyses---handling-nonignorable-missingness-mnar",
    "title": "5  Sensitivity Analyses",
    "section": "5.5 Sensitivity analyses - Handling nonignorable missingness (MNAR)",
    "text": "5.5 Sensitivity analyses - Handling nonignorable missingness (MNAR)\n\nAssumption of MAR is often reasonable, but possibility of data missing not at random (MNAR) is difficult to rule out.\nThus, analysis under MNAR needed.\nAnalysis under MNAR: these methods are heavily assumption driven and the assumptions are not testable as we do not have the missing data.\nConsider a sensitivity analysis framework allowing assessment of robustness of results to the various assumptions.\nMNAR methods: different possibilies e.g. class of pattern-mixture models\nMI can be used to explore departures from MAR (for analysis under a MNAR assumption). This is referred to as controlled MI and includes delta-based MI and reference-based MI (belong to the class of pattern mixture models). Data is imputed under an alternative MNAR distribution that reflects a contextually relevant scenario for the unobserved data. The imputed data sets are then analysed as with standard MI.\n\n\n5.5.1 Reference-based multiple imputation\n\nHas recieved increasing attention in clinical trials as it provides an attractive approach for a sensitivity analysis because missing data assumptions are framed in an intuitive way. The departure from MAR is captured in a qualitative way, making the formulation of the problem intuitive.\nFor example, a plausible MNAR mechanism in a placebo‐controlled trial is to assume that subjects in the experimental arm who dropped out stop taking their treatment and have similar outcomes to those in the placebo arm.\nDifferent options to handle missing outcome data for reference-based imputation were described: e.g. copy reference, copy increments in reference, jump to reference (Paper Carpenter 2013)\n\n\n\n5.5.2 Delta-based multiple imputation\n\nImpute data assuming all unobserved subjects having a poorer or better response than those observed, by adding or subtracting a delta parameter \\(\\delta\\) to the expected value of the MAR imputed values.\nDelta can be implemented in all treatment groups, or in only one group, or may vary by treatment group or an alternative specified factor.\nChoice of values for the sensitivity parameter \\(\\delta\\): often selected by the analyst. Better: selection by content experts.\nSteps: 1. missing values are imputed using standard MI procedure under MAR, 2. imputed values are shifted by adding some fixed value \\(\\delta\\) to reflect the MNAR mechanism, 3. analysis with standard statistical methods including Rubin’s rule to combine results"
  },
  {
    "objectID": "s4_sensitivity_analyses.html#practical-part",
    "href": "s4_sensitivity_analyses.html#practical-part",
    "title": "5  Sensitivity Analyses",
    "section": "5.6 Practical part",
    "text": "5.6 Practical part\n\nTake the high2 data set\nLook again at the complete case analysis\nApply additionally LOCF and compare results\nTry MNAR method reference-based MI and/or delta-based MI. Compare with the other results."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Craig Mallinckrodt, Ilya Lipkovich. 2016. Analyzing Longitudinal\nClinical Trial Data: A Practical Guide. Vol. 1. USA: Chapman;\nHall/CRC. https://doi.org/10.1201/9781315186634.\n\n\nFitzmaurice, Laird, G. M. 2011. Applied Longitudinal Analysis.\nVol. 2. USA: New York, Wiley. https://doi.org/10.1002/9781119513469.\n\n\nMallinckrodt, Craig. 2016. Analyzing Longitudinal Clinical Trial\nData. Chapman; Hall/CRC. https://doi.org/10.1201/9781315186634.\n\n\nRoderick JA Little, Donald B. Rubin. 2019. Statistical Analysis with\nMissing Data. Vol. 3. USA: New York, Wiley. https://doi.org/10.1002/9781119482260."
  }
]